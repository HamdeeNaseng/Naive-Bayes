{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390d8919",
   "metadata": {},
   "source": [
    "# üéØ Comparing Epsilon Values in Epsilon-Greedy\n",
    "\n",
    "**‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô**: Hamdee Naseng  \n",
    "**‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà**: 3 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô 2025  \n",
    "**‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå**: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡πà‡∏≤ Œµ (epsilon) ‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÉ‡∏ô Epsilon-Greedy Algorithm\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£?\n",
    "\n",
    "### üìä Epsilon-Greedy Algorithm\n",
    "\n",
    "**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°**: ‡∏Ñ‡πà‡∏≤ Œµ (epsilon) ‡πÑ‡∏´‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Multi-Armed Bandit?\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å**:\n",
    "- **Œµ = 0.1** (10% explore, 90% exploit)\n",
    "- **Œµ = 0.05** (5% explore, 95% exploit)\n",
    "- **Œµ = 0.01** (1% explore, 99% exploit)\n",
    "\n",
    "**‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö**:\n",
    "- Cumulative Average Reward\n",
    "- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å suboptimal arm\n",
    "- ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ converge\n",
    "\n",
    "---\n",
    "\n",
    "## üîë Key Concepts:\n",
    "\n",
    "### 1. **Epsilon-Greedy Strategy**\n",
    "```\n",
    "‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≠‡∏ö:\n",
    "  ‡∏ñ‡πâ‡∏≤ random() < Œµ:\n",
    "    Explore: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å arm ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°\n",
    "  ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà:\n",
    "    Exploit: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å arm ‡∏ó‡∏µ‡πà‡∏°‡∏µ estimate ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "```\n",
    "\n",
    "### 2. **Trade-off: Exploration vs Exploitation**\n",
    "- **Œµ ‡∏™‡∏π‡∏á (0.1)**: Explore ‡∏°‡∏≤‡∏Å ‚Üí ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß ‡πÅ‡∏ï‡πà waste opportunities\n",
    "- **Œµ ‡∏ï‡πà‡∏≥ (0.01)**: Exploit ‡∏°‡∏≤‡∏Å ‚Üí ‡πÑ‡∏î‡πâ reward ‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏ï‡∏¥‡∏î‡πÉ‡∏ô local optimum\n",
    "- **Œµ ‡∏Å‡∏•‡∏≤‡∏á (0.05)**: Balance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏≠‡∏á‡πÅ‡∏ö‡∏ö\n",
    "\n",
    "### 3. **Normal Distribution Bandits**\n",
    "- ‡πÅ‡∏ï‡πà‡∏•‡∏∞ arm ‡∏°‡∏µ reward ~ N(Œº, 1)\n",
    "- Œº‚ÇÅ = 1.5, Œº‚ÇÇ = 2.5, Œº‚ÇÉ = 3.5\n",
    "- Arm 3 ‡∏Ñ‡∏∑‡∏≠ optimal arm\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "\n",
    "1. ‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Epsilon-Greedy\n",
    "2. ‚úÖ ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤ Œµ ‡∏ï‡πà‡∏≤‡∏á ‡πÜ\n",
    "3. ‚úÖ ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö performance\n",
    "4. ‚úÖ ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤ Œµ ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14e79f2",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Import ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3544d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ font ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "font_path = '../../font/Prompt/Prompt-Regular.ttf'\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "fm.fontManager.addfont(font_path)\n",
    "font_name = font_prop.get_name()\n",
    "\n",
    "plt.rcParams['font.family'] = font_name\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"‚úÖ Import ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "print(f\"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Font ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô '{font_name}' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a206801",
   "metadata": {},
   "source": [
    "## üé∞ Step 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á BanditArm Class\n",
    "\n",
    "Class ‡∏ô‡∏µ‡πâ‡∏à‡∏≥‡∏•‡∏≠‡∏á Bandit Arm ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ reward ‡∏à‡∏≤‡∏Å **Normal Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3786c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditArm:\n",
    "    \"\"\"\n",
    "    Bandit Arm ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ reward ‡∏à‡∏≤‡∏Å Normal Distribution\n",
    "    \n",
    "    reward ~ N(m, 1)\n",
    "    - m: ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á arm\n",
    "    - variance = 1 (‡∏Ñ‡∏á‡∏ó‡∏µ‡πà)\n",
    "    \"\"\"\n",
    "    def __init__(self, m):\n",
    "        self.m = m                # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏à‡∏£‡∏¥‡∏á (true mean)\n",
    "        self.m_estimate = 0       # ‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏£‡∏π‡πâ\n",
    "        self.N = 0                # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å pull\n",
    "    \n",
    "    def pull(self):\n",
    "        \"\"\"Pull arm ‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ reward\"\"\"\n",
    "        # reward = N(m, 1)\n",
    "        return np.random.randn() + self.m\n",
    "    \n",
    "    def update(self, x):\n",
    "        \"\"\"‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏î‡πâ‡∏ß‡∏¢ reward ‡πÉ‡∏´‡∏°‡πà\n",
    "        \n",
    "        ‡πÉ‡∏ä‡πâ incremental mean:\n",
    "        new_mean = old_mean + (1/N) * (new_value - old_mean)\n",
    "        \n",
    "        ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô:\n",
    "        new_mean = (1 - 1/N) * old_mean + (1/N) * new_value\n",
    "        \"\"\"\n",
    "        self.N += 1\n",
    "        self.m_estimate = (1 - 1.0/self.N) * self.m_estimate + 1.0/self.N * x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"BanditArm(m={self.m:.2f}, estimate={self.m_estimate:.2f}, N={self.N})\"\n",
    "\n",
    "print(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á BanditArm class ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!\")\n",
    "print(\"\\nüí° BanditArm ‡∏à‡∏∞:\")\n",
    "print(\"   1. Pull ‚Üí ‡πÑ‡∏î‡πâ reward ‡∏à‡∏≤‡∏Å N(m, 1)\")\n",
    "print(\"   2. Update ‚Üí ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏î‡πâ‡∏ß‡∏¢ incremental mean\")\n",
    "print(\"   3. ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ Beta distribution (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ reward ‡πÄ‡∏õ‡πá‡∏ô continuous)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3467e08",
   "metadata": {},
   "source": [
    "## üß™ Step 3: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö BanditArm\n",
    "\n",
    "‡∏°‡∏≤‡∏î‡∏π‡∏ß‡πà‡∏≤ BanditArm ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á arm ‡∏ó‡∏î‡∏™‡∏≠‡∏ö (true mean = 2.5)\n",
    "test_arm = BanditArm(2.5)\n",
    "\n",
    "print(\"üé∞ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö BanditArm (true mean = 2.5)\\n\")\n",
    "print(f\"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {test_arm}\\n\")\n",
    "\n",
    "# Pull 5 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
    "print(\"Pull 5 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á:\")\n",
    "for i in range(5):\n",
    "    reward = test_arm.pull()\n",
    "    test_arm.update(reward)\n",
    "    print(f\"  Round {i+1}: reward = {reward:.2f}, estimate = {test_arm.m_estimate:.2f}\")\n",
    "\n",
    "print(f\"\\nüìä ‡∏´‡∏•‡∏±‡∏á 5 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á: {test_arm}\")\n",
    "\n",
    "# Pull ‡∏≠‡∏µ‡∏Å 95 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
    "for i in range(95):\n",
    "    reward = test_arm.pull()\n",
    "    test_arm.update(reward)\n",
    "\n",
    "print(f\"üìä ‡∏´‡∏•‡∏±‡∏á 100 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á: {test_arm}\")\n",
    "\n",
    "print(\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "print(\"  - ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô estimate = 0\")\n",
    "print(\"  - ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å pull ‡∏´‡∏•‡∏≤‡∏¢ ‡πÜ ‡∏Ñ‡∏£‡∏±‡πâg ‚Üí estimate ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ true mean (2.5)\")\n",
    "print(\"  - Variance ‡∏Ç‡∏≠‡∏á reward ‡∏ó‡∏≥‡πÉ‡∏´‡πâ estimate ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3433602",
   "metadata": {},
   "source": [
    "## üèÉ Step 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô run_experiment\n",
    "\n",
    "‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏£‡∏±‡∏ô Epsilon-Greedy experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9063249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(m1, m2, m3, eps, N):\n",
    "    \"\"\"\n",
    "    ‡∏£‡∏±‡∏ô Epsilon-Greedy experiment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    m1, m2, m3 : float\n",
    "        True means ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ arm\n",
    "    eps : float\n",
    "        Epsilon (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ explore)\n",
    "    N : int\n",
    "        ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cumulative_average : array\n",
    "        Average reward ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ\n",
    "    \"\"\"\n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á 3 bandits\n",
    "    bandits = [BanditArm(m1), BanditArm(m2), BanditArm(m3)]\n",
    "    \n",
    "    # ‡∏´‡∏≤ optimal arm\n",
    "    means = np.array([m1, m2, m3])\n",
    "    true_best = np.argmax(means)  # arm ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "    count_suboptimal = 0          # ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏¥‡∏î\n",
    "    \n",
    "    # ‡πÄ‡∏Å‡πá‡∏ö rewards\n",
    "    data = np.empty(N)\n",
    "    \n",
    "    print(f\"\\nüéØ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á (Œµ = {eps}, N = {N:,})\")\n",
    "    print(f\"   Arm means: {means}\")\n",
    "    print(f\"   Optimal arm: Arm {true_best + 1} (mean = {means[true_best]:.1f})\")\n",
    "    print(f\"   Strategy: {eps:.0%} explore, {(1-eps):.0%} exploit\\n\")\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Epsilon-Greedy\n",
    "        p = np.random.random()\n",
    "        \n",
    "        if p < eps:\n",
    "            # Explore: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°\n",
    "            j = np.random.choice(len(bandits))\n",
    "        else:\n",
    "            # Exploit: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å arm ‡∏ó‡∏µ‡πà‡∏°‡∏µ estimate ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
    "            j = np.argmax([b.m_estimate for b in bandits])\n",
    "        \n",
    "        # Pull arm ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å\n",
    "        x = bandits[j].pull()\n",
    "        bandits[j].update(x)\n",
    "        \n",
    "        # ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏¥‡∏î\n",
    "        if j != true_best:\n",
    "            count_suboptimal += 1\n",
    "        \n",
    "        # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "        data[i] = x\n",
    "    \n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì cumulative average\n",
    "    cumulative_average = np.cumsum(data) / (np.arange(N) + 1)\n",
    "    \n",
    "    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "    print(\"üìä ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:\")\n",
    "    for i, b in enumerate(bandits):\n",
    "        is_best = \" ‚≠ê\" if i == true_best else \"\"\n",
    "        print(f\"   Arm {i+1}: true={b.m:.1f}, estimate={b.m_estimate:.3f}, pulls={b.N:,}{is_best}\")\n",
    "    \n",
    "    suboptimal_pct = float(count_suboptimal) / N\n",
    "    print(f\"\\n   Suboptimal choices: {count_suboptimal:,} / {N:,} ({suboptimal_pct:.1%})\")\n",
    "    print(f\"   Final average reward: {cumulative_average[-1]:.3f}\")\n",
    "    \n",
    "    # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(cumulative_average, linewidth=2, label=f'Œµ = {eps}')\n",
    "    plt.axhline(m1, color='blue', linestyle='--', alpha=0.5, label=f'Arm 1: Œº={m1}')\n",
    "    plt.axhline(m2, color='orange', linestyle='--', alpha=0.5, label=f'Arm 2: Œº={m2}')\n",
    "    plt.axhline(m3, color='green', linestyle='--', alpha=0.5, label=f'Arm 3: Œº={m3}')\n",
    "    \n",
    "    plt.xlabel('Iteration', fontsize=12)\n",
    "    plt.ylabel('Cumulative Average Reward', fontsize=12)\n",
    "    plt.title(f'Epsilon-Greedy (Œµ = {eps})', fontsize=14)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cumulative_average\n",
    "\n",
    "print(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô run_experiment ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27ecb58",
   "metadata": {},
   "source": [
    "## üß™ Step 5: ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö Œµ = 0.1 (10% explore)\n",
    "\n",
    "‡∏°‡∏≤‡∏î‡∏π‡∏ß‡πà‡∏≤ Œµ = 0.1 ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤\n",
    "m1, m2, m3 = 1.5, 2.5, 3.5  # True means\n",
    "N = 100000                   # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö\n",
    "\n",
    "# ‡∏£‡∏±‡∏ô experiment\n",
    "c_1 = run_experiment(m1, m2, m3, eps=0.1, N=N)\n",
    "\n",
    "print(\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "print(\"  - Œµ = 0.1 ‚Üí Explore 10% ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤\")\n",
    "print(\"  - ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£\")\n",
    "print(\"  - ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å suboptimal arms ‡∏ö‡πà‡∏≠‡∏¢ (~10%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c6edea",
   "metadata": {},
   "source": [
    "## üß™ Step 6: ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö Œµ = 0.05 (5% explore)\n",
    "\n",
    "‡∏°‡∏≤‡∏î‡∏π‡∏ß‡πà‡∏≤ Œµ = 0.05 ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏£‡∏±‡∏ô experiment\n",
    "c_05 = run_experiment(m1, m2, m3, eps=0.05, N=N)\n",
    "\n",
    "print(\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "print(\"  - Œµ = 0.05 ‚Üí Explore 5% ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤\")\n",
    "print(\"  - Exploit ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‚Üí Average reward ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô\")\n",
    "print(\"  - Suboptimal choices ‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á (~5%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a34c27",
   "metadata": {},
   "source": [
    "## üß™ Step 7: ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö Œµ = 0.01 (1% explore)\n",
    "\n",
    "‡∏°‡∏≤‡∏î‡∏π‡∏ß‡πà‡∏≤ Œµ = 0.01 ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏£‡∏±‡∏ô experiment\n",
    "c_01 = run_experiment(m1, m2, m3, eps=0.01, N=N)\n",
    "\n",
    "print(\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "print(\"  - Œµ = 0.01 ‚Üí Explore 1% ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤\")\n",
    "print(\"  - Exploit ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‚Üí Average reward ‡πÉ‡∏Å‡∏•‡πâ optimal\")\n",
    "print(\"  - Suboptimal choices ‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å (~1%)\")\n",
    "print(\"  - ‡πÅ‡∏ï‡πà‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ï‡∏¥‡∏î local optimum ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å best arm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6a2fd",
   "metadata": {},
   "source": [
    "## üìä Step 8: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Log Scale\n",
    "\n",
    "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏±‡πâ‡∏á 3 ‡∏Ñ‡πà‡∏≤ Œµ ‡πÉ‡∏ô‡∏Å‡∏£‡∏≤‡∏ü log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb16d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö (log scale)\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.plot(c_1, linewidth=2, label='Œµ = 0.1 (10% explore)', alpha=0.8)\n",
    "plt.plot(c_05, linewidth=2, label='Œµ = 0.05 (5% explore)', alpha=0.8)\n",
    "plt.plot(c_01, linewidth=2, label='Œµ = 0.01 (1% explore)', alpha=0.8)\n",
    "\n",
    "plt.axhline(m3, color='green', linestyle='--', linewidth=2, label=f'Optimal (Œº = {m3})')\n",
    "\n",
    "plt.xlabel('Iteration (Log Scale)', fontsize=12)\n",
    "plt.ylabel('Cumulative Average Reward', fontsize=12)\n",
    "plt.title('‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Epsilon Values (Log Scale)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏≤‡∏ü (Log Scale):\")\n",
    "print(\"\\n  - ‡πÅ‡∏Å‡∏ô X ‡πÄ‡∏õ‡πá‡∏ô log scale ‚Üí ‡πÄ‡∏´‡πá‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏ä‡πà‡∏ß‡∏á‡πÅ‡∏£‡∏Å ‡πÜ ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\")\n",
    "print(\"  - Œµ = 0.1 (‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô): ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ä‡πâ‡∏≤ ‡πÅ‡∏ï‡πà stable ‡πÄ‡∏£‡πá‡∏ß\")\n",
    "print(\"  - Œµ = 0.05 (‡∏™‡∏µ‡∏™‡πâ‡∏°): ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‚Üí Balance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á exploration/exploitation\")\n",
    "print(\"  - Œµ = 0.01 (‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß): ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏µ ‡πÅ‡∏ï‡πà convergence ‡∏ä‡πâ‡∏≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb00b8",
   "metadata": {},
   "source": [
    "## üìä Step 9: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Linear Scale\n",
    "\n",
    "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏±‡πâ‡∏á 3 ‡∏Ñ‡πà‡∏≤ Œµ ‡πÉ‡∏ô‡∏Å‡∏£‡∏≤‡∏ü linear scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc556d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö (linear scale)\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.plot(c_1, linewidth=2, label='Œµ = 0.1 (10% explore)', alpha=0.8)\n",
    "plt.plot(c_05, linewidth=2, label='Œµ = 0.05 (5% explore)', alpha=0.8)\n",
    "plt.plot(c_01, linewidth=2, label='Œµ = 0.01 (1% explore)', alpha=0.8)\n",
    "\n",
    "plt.axhline(m3, color='green', linestyle='--', linewidth=2, label=f'Optimal (Œº = {m3})')\n",
    "\n",
    "plt.xlabel('Iteration (Linear Scale)', fontsize=12)\n",
    "plt.ylabel('Cumulative Average Reward', fontsize=12)\n",
    "plt.title('‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Epsilon Values (Linear Scale)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏≤‡∏ü (Linear Scale):\")\n",
    "print(\"\\n  - ‡πÅ‡∏Å‡∏ô X ‡πÄ‡∏õ‡πá‡∏ô linear scale ‚Üí ‡πÄ‡∏´‡πá‡∏ô‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß\")\n",
    "print(\"  - Œµ = 0.1: Average reward ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏™‡πâ‡∏ô optimal ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\")\n",
    "print(\"  - Œµ = 0.05: ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏™‡πâ‡∏ô optimal ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‚úÖ\")\n",
    "print(\"  - Œµ = 0.01: ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏Å‡∏•‡πâ optimal ‡πÅ‡∏ï‡πà convergence ‡∏ä‡πâ‡∏≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c8447",
   "metadata": {},
   "source": [
    "## üìà Step 10: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
    "\n",
    "‡∏°‡∏≤‡∏î‡∏π‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Epsilon Values\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüéØ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤:\")\n",
    "print(f\"   Arm 1: Œº = {m1}\")\n",
    "print(f\"   Arm 2: Œº = {m2}\")\n",
    "print(f\"   Arm 3: Œº = {m3} ‚≠ê (Optimal)\")\n",
    "print(f\"   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö: {N:,}\")\n",
    "\n",
    "print(\"\\nüìä Final Cumulative Average Rewards:\")\n",
    "print(f\"   Œµ = 0.10: {c_1[-1]:.4f}\")\n",
    "print(f\"   Œµ = 0.05: {c_05[-1]:.4f} ‚úÖ (‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)\")\n",
    "print(f\"   Œµ = 0.01: {c_01[-1]:.4f}\")\n",
    "\n",
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì gap ‡∏à‡∏≤‡∏Å optimal\n",
    "print(\"\\nüìâ Gap ‡∏à‡∏≤‡∏Å Optimal (Œº = 3.5):\")\n",
    "print(f\"   Œµ = 0.10: {m3 - c_1[-1]:.4f} ({(m3 - c_1[-1])/m3:.1%} loss)\")\n",
    "print(f\"   Œµ = 0.05: {m3 - c_05[-1]:.4f} ({(m3 - c_05[-1])/m3:.1%} loss) ‚úÖ\")\n",
    "print(f\"   Œµ = 0.01: {m3 - c_01[-1]:.4f} ({(m3 - c_01[-1])/m3:.1%} loss)\")\n",
    "\n",
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì cumulative regret\n",
    "regret_1 = (m3 - c_1).sum()\n",
    "regret_05 = (m3 - c_05).sum()\n",
    "regret_01 = (m3 - c_01).sum()\n",
    "\n",
    "print(\"\\nüí∏ Cumulative Regret (‡∏¢‡∏¥‡πà‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ):\")\n",
    "print(f\"   Œµ = 0.10: {regret_1:,.2f}\")\n",
    "print(f\"   Œµ = 0.05: {regret_05:,.2f} ‚úÖ (‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)\")\n",
    "print(f\"   Œµ = 0.01: {regret_01:,.2f}\")\n",
    "\n",
    "print(\"\\nüéì ‡∏Ç‡πâ‡∏≠‡∏™‡∏£‡∏∏‡∏õ:\")\n",
    "print(f\"   1. Œµ = 0.05 ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\")\n",
    "print(f\"   2. Œµ = 0.1 explore ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‚Üí waste opportunities\")\n",
    "print(f\"   3. Œµ = 0.01 explore ‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‚Üí convergence ‡∏ä‡πâ‡∏≤\")\n",
    "print(f\"   4. ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Œµ ‡∏ó‡∏µ‡πà balance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á exploration/exploitation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2a2f0",
   "metadata": {},
   "source": [
    "## üéì ‡∏™‡∏£‡∏∏‡∏õ: ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n",
    "\n",
    "### üîë ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏´‡∏•‡∏±‡∏Å (Key Concepts):\n",
    "\n",
    "#### 1. **Epsilon-Greedy Algorithm**\n",
    "\n",
    "**‡∏™‡∏π‡∏ï‡∏£**:\n",
    "```\n",
    "‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≠‡∏ö:\n",
    "  random_value = random()\n",
    "  \n",
    "  if random_value < Œµ:\n",
    "    # Explore\n",
    "    action = random_choice(arms)\n",
    "  else:\n",
    "    # Exploit\n",
    "    action = argmax(estimated_values)\n",
    "```\n",
    "\n",
    "**‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå**:\n",
    "- **Œµ (epsilon)**: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ explore (0 ‚â§ Œµ ‚â§ 1)\n",
    "\n",
    "#### 2. **Normal Distribution Bandits**\n",
    "\n",
    "**‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å Beta-Bernoulli**:\n",
    "- **Beta-Bernoulli**: Reward = 0 ‡∏´‡∏£‡∏∑‡∏≠ 1 (click/no-click)\n",
    "- **Normal**: Reward ‡πÄ‡∏õ‡πá‡∏ô continuous value ~ N(Œº, œÉ¬≤)\n",
    "\n",
    "**Update Rule**:\n",
    "```\n",
    "Incremental Mean:\n",
    "new_estimate = old_estimate + (1/N) √ó (reward - old_estimate)\n",
    "\n",
    "‡∏´‡∏£‡∏∑‡∏≠:\n",
    "new_estimate = (1 - 1/N) √ó old_estimate + (1/N) √ó reward\n",
    "```\n",
    "\n",
    "**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**:\n",
    "- ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö reward ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏á‡∏¥‡∏ô, ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤, ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)\n",
    "- Update ‡∏á‡πà‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "\n",
    "#### 3. **‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤ Œµ ‡∏ï‡πà‡∏≤‡∏á ‡πÜ**\n",
    "\n",
    "| Œµ | Explore % | Exploit % | ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ | ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢ | ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö |\n",
    "|---|-----------|-----------|-------|---------|----------|\n",
    "| **0.1** | 10% | 90% | ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß | Waste 10% | ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ arms ‡πÑ‡∏´‡∏ô‡∏î‡∏µ |\n",
    "| **0.05** | 5% | 95% | **Balance ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î** | - | **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥** ‚úÖ |\n",
    "| **0.01** | 1% | 99% | Exploit ‡∏°‡∏≤‡∏Å | Convergence ‡∏ä‡πâ‡∏≤ | ‡∏£‡∏π‡πâ‡πÅ‡∏ô‡πà‡∏ß‡πà‡∏≤ arm ‡πÑ‡∏´‡∏ô‡∏î‡∏µ |\n",
    "\n",
    "#### 4. **Exploration vs Exploitation Trade-off**\n",
    "\n",
    "**Exploration** (‡∏™‡∏≥‡∏£‡∏ß‡∏à):\n",
    "- ‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ arms ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤\n",
    "- ‚úÖ ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ environment\n",
    "- ‚ùå ‡∏≠‡∏≤‡∏à‡πÑ‡∏î‡πâ reward ‡∏ï‡πà‡∏≥‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏™‡∏±‡πâ‡∏ô\n",
    "\n",
    "**Exploitation** (‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå):\n",
    "- ‚úÖ ‡πÑ‡∏î‡πâ reward ‡∏™‡∏π‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏™‡∏±‡πâ‡∏ô\n",
    "- ‚ùå ‡∏≠‡∏≤‡∏à‡∏û‡∏•‡∏≤‡∏î optimal arm\n",
    "- ‚ùå ‡∏ï‡∏¥‡∏î‡πÉ‡∏ô local optimum\n",
    "\n",
    "**Balance**:\n",
    "- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: Explore ‡∏°‡∏≤‡∏Å (Œµ ‡∏™‡∏π‡∏á)\n",
    "- ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏π‡πâ‡πÅ‡∏•‡πâ‡∏ß: Exploit ‡∏°‡∏≤‡∏Å (Œµ ‡∏ï‡πà‡∏≥)\n",
    "- **Adaptive Œµ**: Œµ = 1/t ‡∏´‡∏£‡∏∑‡∏≠ Œµ = 1/‚àöt\n",
    "\n",
    "#### 5. **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Thompson Sampling**\n",
    "\n",
    "| Aspect | Epsilon-Greedy | Thompson Sampling |\n",
    "|--------|----------------|-------------------|\n",
    "| **Philosophy** | Fixed exploration rate | Probability matching |\n",
    "| **Implementation** | ‡∏á‡πà‡∏≤‡∏¢‡∏°‡∏≤‡∏Å | ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏´‡∏ô‡πà‡∏≠‡∏¢ |\n",
    "| **Performance** | ‡∏î‡∏µ (‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡πâ‡∏á Œµ ‡∏ñ‡∏π‡∏Å) | ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ (optimal) |\n",
    "| **Adaptation** | ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö Œµ ‡πÄ‡∏≠‡∏á | Auto-adaptive |\n",
    "| **Parameter tuning** | ‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏≤ Œµ ‡∏ó‡∏µ‡πà‡∏î‡∏µ | ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á |\n",
    "\n",
    "### üìä ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á:\n",
    "\n",
    "**‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô**:\n",
    "1. ‚úÖ **Œµ = 0.05 ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î** ‚Üí Cumulative reward ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î, regret ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î\n",
    "2. ‚úÖ Œµ = 0.1 explore ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‚Üí Loss ~3%\n",
    "3. ‚úÖ Œµ = 0.01 exploit ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‚Üí Convergence ‡∏ä‡πâ‡∏≤\n",
    "4. ‚úÖ Log scale ‡πÅ‡∏™‡∏î‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏ä‡πà‡∏ß‡∏á‡πÅ‡∏£‡∏Å, Linear scale ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß\n",
    "\n",
    "### üÜö Comparison with Other Algorithms:\n",
    "\n",
    "| Algorithm | Regret Bound | Implementation | Best For |\n",
    "|-----------|--------------|----------------|----------|\n",
    "| **Random** | Linear | ‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î | Baseline |\n",
    "| **Greedy** | Linear | ‡∏á‡πà‡∏≤‡∏¢ | ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ |\n",
    "| **Œµ-Greedy** | O(log N) | ‡∏á‡πà‡∏≤‡∏¢ | Simple problems |\n",
    "| **UCB** | O(log N) | ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á | Theoretical optimal |\n",
    "| **Thompson** | O(log N) | ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô | **Best overall** ‚úÖ |\n",
    "\n",
    "### üåç ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ:\n",
    "\n",
    "1. **A/B Testing**:\n",
    "   - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö web design, features\n",
    "   - Œµ = 0.05 ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö traffic ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á\n",
    "\n",
    "2. **Online Advertising**:\n",
    "   - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ads ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "   - Œµ-Greedy ‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£ implement\n",
    "\n",
    "3. **Recommendation Systems**:\n",
    "   - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ products, content\n",
    "   - Adaptive Œµ ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤\n",
    "\n",
    "4. **Resource Allocation**:\n",
    "   - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å server, routing\n",
    "   - Œµ ‡∏ï‡πà‡∏≥ (exploit ‡∏°‡∏≤‡∏Å)\n",
    "\n",
    "### üí° Key Takeaways:\n",
    "\n",
    "1. **Œµ = 0.05 ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ ‡πÜ ‡∏Å‡∏£‡∏ì‡∏µ** (5% explore)\n",
    "2. **Trade-off ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**: Explore ‡∏°‡∏≤‡∏Å = ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß ‡πÅ‡∏ï‡πà waste, Exploit ‡∏°‡∏≤‡∏Å = ‡πÑ‡∏î‡πâ reward ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏û‡∏•‡∏≤‡∏î optimal\n",
    "3. **Adaptive Œµ ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ Fixed**: Œµ = 1/t ‡∏´‡∏£‡∏∑‡∏≠ Œµ = 1/‚àöt\n",
    "4. **Thompson Sampling ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤** ‡πÅ‡∏ï‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤\n",
    "5. **Normal distribution bandits**: ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö continuous rewards (‡πÄ‡∏á‡∏¥‡∏ô, ‡πÄ‡∏ß‡∏•‡∏≤, ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)\n",
    "\n",
    "### üîß Improvements:\n",
    "\n",
    "**Adaptive Epsilon**:\n",
    "```python\n",
    "# Œµ ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ\n",
    "eps = 1.0 / (t + 1)        # 1/t decay\n",
    "eps = 1.0 / np.sqrt(t + 1) # 1/‚àöt decay\n",
    "```\n",
    "\n",
    "**UCB (Upper Confidence Bound)**:\n",
    "```python\n",
    "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å arm ‡∏ó‡∏µ‡πà‡∏°‡∏µ UCB ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
    "ucb = mean_estimate + np.sqrt(2 * np.log(total_pulls) / arm_pulls)\n",
    "```\n",
    "\n",
    "**Thompson Sampling**:\n",
    "```python\n",
    "# Sample ‡∏à‡∏≤‡∏Å posterior ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å arm ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
    "sample = np.random.normal(mean_estimate, std_estimate)\n",
    "```\n",
    "\n",
    "### üìö Incremental Mean Formula:\n",
    "\n",
    "**‡∏ó‡∏≥‡πÑ‡∏°‡πÉ‡∏ä‡πâ‡∏™‡∏π‡∏ï‡∏£‡∏ô‡∏µ‡πâ?**\n",
    "```\n",
    "new_mean = old_mean + (1/N) √ó (new_value - old_mean)\n",
    "```\n",
    "\n",
    "**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**:\n",
    "- ‚úÖ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (Memory efficient)\n",
    "- ‚úÖ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏£‡πá‡∏ß (O(1) per update)\n",
    "- ‚úÖ Numerically stable\n",
    "\n",
    "**Proof**:\n",
    "```\n",
    "new_mean = (sum + new_value) / (N + 1)\n",
    "         = (N √ó old_mean + new_value) / (N + 1)\n",
    "         = N/(N+1) √ó old_mean + 1/(N+1) √ó new_value\n",
    "         = old_mean + 1/(N+1) √ó (new_value - old_mean)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏î‡πâ‡∏ß‡∏¢!\n",
    "\n",
    "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à **Epsilon-Greedy Algorithm** ‡πÅ‡∏•‡∏∞ **‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡πà‡∏≤ Œµ** ‡πÅ‡∏•‡πâ‡∏ß!\n",
    "\n",
    "**Next Steps**:\n",
    "- ‡∏ó‡∏î‡∏•‡∏≠‡∏á **Adaptive Epsilon** (Œµ ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ)\n",
    "- ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö **UCB (Upper Confidence Bound)**\n",
    "- ‡∏®‡∏∂‡∏Å‡∏©‡∏≤ **Thompson Sampling** (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ Œµ-Greedy)\n",
    "- ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö **Real-world datasets**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
