{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba0ca98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Font ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô 'Prompt' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ font ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "font_path = '../../font/Prompt/Prompt-Regular.ttf'\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "fm.fontManager.addfont(font_path)\n",
    "font_name = font_prop.get_name()\n",
    "plt.rcParams['font.family'] = font_name\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Font ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô '{font_name}' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e2336",
   "metadata": {},
   "source": [
    "# üî¢ Gaussian Naive Bayes Implementation from Scratch\n",
    "\n",
    "**‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå**: ‡∏™‡∏£‡πâ‡∏≤‡∏á Gaussian Naive Bayes ‡∏à‡∏≤‡∏Å‡∏®‡∏π‡∏ô‡∏¢‡πå (From Scratch) ‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö MNIST Dataset\n",
    "\n",
    "---\n",
    "\n",
    "## üìã ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå:\n",
    "\n",
    "### üéØ **‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢**:\n",
    "- ‡∏™‡∏£‡πâ‡∏≤‡∏á Gaussian Naive Bayes Algorithm ‡πÄ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ sklearn\n",
    "- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡∏≠‡∏á Gaussian NB\n",
    "- ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö MNIST (‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 0-9)\n",
    "\n",
    "### üßÆ **Dataset**: MNIST Handwritten Digits\n",
    "- **Training**: 60,000 ‡∏†‡∏≤‡∏û\n",
    "- **Testing**: 10,000 ‡∏†‡∏≤‡∏û\n",
    "- **‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û**: 28√ó28 pixels = 784 features\n",
    "- **Classes**: 10 classes (‡πÄ‡∏•‡∏Ç 0-9)\n",
    "\n",
    "### üí° **‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏á?**\n",
    "- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏•‡∏∂‡∏Å‡∏ã‡∏∂‡πâ‡∏á\n",
    "- ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "- ‡∏ù‡∏∂‡∏Å‡∏ó‡∏±‡∏Å‡∏©‡∏∞ implementation\n",
    "\n",
    "---\n",
    "\n",
    "## üó∫Ô∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:\n",
    "\n",
    "1. **Import Libraries** üìö\n",
    "2. **Implement Gaussian NB Class** üî® (from scratch)\n",
    "3. **Load MNIST Data** üì•\n",
    "4. **Preprocess Data** üîß (reshape, normalize)\n",
    "5. **Train Model** üèãÔ∏è\n",
    "6. **Evaluate Performance** üéØ (accuracy, speed)\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ ‡∏™‡∏π‡∏ï‡∏£‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå:\n",
    "\n",
    "### Bayes' Theorem:\n",
    "$$P(y|X) = \\frac{P(X|y) \\times P(y)}{P(X)}$$\n",
    "\n",
    "### Gaussian NB (Continuous Features):\n",
    "$$P(X|y) = \\prod_{i=1}^{D} \\frac{1}{\\sqrt{2\\pi\\sigma_{yi}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{yi})^2}{2\\sigma_{yi}^2}\\right)$$\n",
    "\n",
    "**Where**:\n",
    "- $\\mu_{yi}$ = Mean ‡∏Ç‡∏≠‡∏á feature $i$ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö class $y$\n",
    "- $\\sigma_{yi}^2$ = Variance ‡∏Ç‡∏≠‡∏á feature $i$ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö class $y$\n",
    "- $D$ = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features (784 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö MNIST)\n",
    "\n",
    "### Log-space (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Underflow):\n",
    "$$\\log P(y|X) = \\log P(X|y) + \\log P(y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ce5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB:\n",
    "  def __init__(self, smoothing=1e-3):\n",
    "    self.smoothing = smoothing\n",
    "  \n",
    "  def fit(self, X, Y):\n",
    "    K = len(set(Y))\n",
    "    N, D = X.shape\n",
    "\n",
    "    self.logpriors = np.zeros(K)\n",
    "    self.means = np.zeros((K, D))\n",
    "    self.vars = np.zeros((K, D))\n",
    "\n",
    "    for k in range(K):\n",
    "      # prior - log(Nk / N) = log(Nk) - log(N)\n",
    "      self.logpriors[k] = np.log(len(Y[Y == k])) - np.log(N)\n",
    "\n",
    "      # likelihood\n",
    "      Xk = X[Y == k]\n",
    "      self.means[k] = Xk.mean(axis=0)\n",
    "      self.vars[k] = Xk.var(axis=0) + self.smoothing\n",
    "  \n",
    "  def score(self, X, Y):\n",
    "    P = self.predict(X)\n",
    "    return np.mean(P == Y)\n",
    "  \n",
    "  def predict(self, X):\n",
    "    N, D = X.shape\n",
    "    K = len(self.logpriors)\n",
    "    P = np.zeros((N, K))\n",
    "\n",
    "    for k, pr, m, v in zip(range(K), self.logpriors, self.means, self.vars):\n",
    "      P[:, k] = mvn.logpdf(X, mean=m, cov=v) + pr\n",
    "\n",
    "    return np.argmax(P, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9dce7d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Step 1: Import Libraries\n",
    "\n",
    "‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° libraries ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "930f0f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading MNIST Dataset...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "\n",
      "‚úÖ Dataset Loaded Successfully!\n",
      "============================================================\n",
      "üìä Dataset Information:\n",
      "   ‚Ä¢ Training Images: 60,000\n",
      "   ‚Ä¢ Testing Images:  10,000\n",
      "   ‚Ä¢ Image Shape: 28 √ó 28 pixels\n",
      "   ‚Ä¢ Total Pixels per Image: 784\n",
      "   ‚Ä¢ Pixel Value Range: 0 - 255\n",
      "   ‚Ä¢ Classes: 10 classes (digits 0-9)\n",
      "\n",
      "üí° ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\n",
      "   ‚Ä¢ X_train shape: (60000, 28, 28) = (samples, height, width)\n",
      "   ‚Ä¢ y_train shape: (60000,) = (samples,)\n",
      "   ‚Ä¢ X_test shape:  (10000, 28, 28)\n",
      "   ‚Ä¢ y_test shape:  (10000,)\n",
      "\n",
      "‚úÖ Dataset Loaded Successfully!\n",
      "============================================================\n",
      "üìä Dataset Information:\n",
      "   ‚Ä¢ Training Images: 60,000\n",
      "   ‚Ä¢ Testing Images:  10,000\n",
      "   ‚Ä¢ Image Shape: 28 √ó 28 pixels\n",
      "   ‚Ä¢ Total Pixels per Image: 784\n",
      "   ‚Ä¢ Pixel Value Range: 0 - 255\n",
      "   ‚Ä¢ Classes: 10 classes (digits 0-9)\n",
      "\n",
      "üí° ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\n",
      "   ‚Ä¢ X_train shape: (60000, 28, 28) = (samples, height, width)\n",
      "   ‚Ä¢ y_train shape: (60000,) = (samples,)\n",
      "   ‚Ä¢ X_test shape:  (10000, 28, 28)\n",
      "   ‚Ä¢ y_test shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "print(\"üì• Loading MNIST Dataset...\")\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"\\n‚úÖ Dataset Loaded Successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä Dataset Information:\")\n",
    "print(f\"   ‚Ä¢ Training Images: {X_train.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Testing Images:  {X_test.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Image Shape: {X_train.shape[1]} √ó {X_train.shape[2]} pixels\")\n",
    "print(f\"   ‚Ä¢ Total Pixels per Image: {X_train.shape[1] * X_train.shape[2]}\")\n",
    "print(f\"   ‚Ä¢ Pixel Value Range: {X_train.min()} - {X_train.max()}\")\n",
    "print(f\"   ‚Ä¢ Classes: {len(set(y_train))} classes (digits 0-9)\")\n",
    "\n",
    "print(\"\\nüí° ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\")\n",
    "print(\"   ‚Ä¢ X_train shape:\", X_train.shape, \"= (samples, height, width)\")\n",
    "print(\"   ‚Ä¢ y_train shape:\", y_train.shape, \"= (samples,)\")\n",
    "print(\"   ‚Ä¢ X_test shape: \", X_test.shape)\n",
    "print(\"   ‚Ä¢ y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a2a83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî® Step 2: Implement Gaussian Naive Bayes Class\n",
    "\n",
    "### üß© ‡∏™‡∏£‡πâ‡∏≤‡∏á Class ‡∏à‡∏≤‡∏Å‡∏®‡∏π‡∏ô‡∏¢‡πå (From Scratch)\n",
    "\n",
    "**Components**:\n",
    "\n",
    "1. **`__init__()`**: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î smoothing parameter\n",
    "   - `smoothing`: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô variance = 0 (‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢ 0)\n",
    "\n",
    "2. **`fit()`**: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "   - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì **Prior**: $P(y_k) = \\frac{N_k}{N}$\n",
    "   - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì **Mean**: $\\mu_{k,d} = \\text{mean}(X_{k,d})$\n",
    "   - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì **Variance**: $\\sigma_{k,d}^2 = \\text{var}(X_{k,d})$\n",
    "\n",
    "3. **`predict()`**: ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ class\n",
    "   - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì log probability ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å class\n",
    "   - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å class ‡∏ó‡∏µ‡πà‡∏°‡∏µ probability ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
    "\n",
    "4. **`score()`**: ‡∏ß‡∏±‡∏î accuracy\n",
    "   - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö prediction vs true labels\n",
    "\n",
    "**Key Concepts**:\n",
    "- ‡πÉ‡∏ä‡πâ **log-space** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô numerical underflow\n",
    "- ‡πÉ‡∏ä‡πâ **multivariate_normal.logpdf()** ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì log-likelihood\n",
    "- Assume features ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô (Naive Assumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d79162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Preprocessing Data...\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Reshape (2D ‚Üí 1D):\n",
      "   Before: (60000, 28, 28) = (samples, height, width)\n",
      "   After:  (60000, 784) = (samples, features)\n",
      "\n",
      "2Ô∏è‚É£ Normalize (Scale to [0, 1]):\n",
      "   Before: min=255, max=255\n",
      "   After:  min=0.0, max=1.0\n",
      "\n",
      "‚úÖ Preprocessing Complete!\n",
      "============================================================\n",
      "üìä Final Shape:\n",
      "   ‚Ä¢ X_train: (60000, 784) = (60,000 images √ó 784 features)\n",
      "   ‚Ä¢ X_test:  (10000, 784)  = (10,000 images √ó 784 features)\n",
      "\n",
      "üí° ‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature = ‡∏Ñ‡πà‡∏≤ pixel ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß (0.0 - 1.0)\n",
      "   ‚Ä¢ 0.0 = ‡∏™‡∏µ‡∏î‡∏≥ (background)\n",
      "   ‚Ä¢ 1.0 = ‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô)\n",
      "   After:  min=0.0, max=1.0\n",
      "\n",
      "‚úÖ Preprocessing Complete!\n",
      "============================================================\n",
      "üìä Final Shape:\n",
      "   ‚Ä¢ X_train: (60000, 784) = (60,000 images √ó 784 features)\n",
      "   ‚Ä¢ X_test:  (10000, 784)  = (10,000 images √ó 784 features)\n",
      "\n",
      "üí° ‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature = ‡∏Ñ‡πà‡∏≤ pixel ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß (0.0 - 1.0)\n",
      "   ‚Ä¢ 0.0 = ‡∏™‡∏µ‡∏î‡∏≥ (background)\n",
      "   ‚Ä¢ 1.0 = ‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô)\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Preprocessing Data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reshape: ‡πÅ‡∏õ‡∏•‡∏á 28√ó28 ‚Üí 784 features\n",
    "print(\"\\n1Ô∏è‚É£ Reshape (2D ‚Üí 1D):\")\n",
    "print(f\"   Before: {X_train.shape} = (samples, height, width)\")\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "print(f\"   After:  {X_train.shape} = (samples, features)\")\n",
    "\n",
    "# Normalize: [0, 255] ‚Üí [0, 1]\n",
    "print(\"\\n2Ô∏è‚É£ Normalize (Scale to [0, 1]):\")\n",
    "print(f\"   Before: min={X_train.max()}, max={X_train.max()}\")\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255.\n",
    "print(f\"   After:  min={X_train.min():.1f}, max={X_train.max():.1f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Preprocessing Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä Final Shape:\")\n",
    "print(f\"   ‚Ä¢ X_train: {X_train.shape} = (60,000 images √ó 784 features)\")\n",
    "print(f\"   ‚Ä¢ X_test:  {X_test.shape}  = (10,000 images √ó 784 features)\")\n",
    "print(f\"\\nüí° ‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature = ‡∏Ñ‡πà‡∏≤ pixel ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß (0.0 - 1.0)\")\n",
    "print(f\"   ‚Ä¢ 0.0 = ‡∏™‡∏µ‡∏î‡∏≥ (background)\")\n",
    "print(f\"   ‚Ä¢ 1.0 = ‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4228c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• Step 3: Load MNIST Dataset\n",
    "\n",
    "‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• MNIST ‡∏à‡∏≤‡∏Å Keras (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 0-9 ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏°‡∏∑‡∏≠)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "203136ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training Gaussian Naive Bayes Model...\n",
      "======================================================================\n",
      "‚úÖ Model Created with smoothing = 0.01\n",
      "\n",
      "‚è±Ô∏è Training Phase:\n",
      "   ‚Ä¢ Fit Duration: 0:00:00.508681\n",
      "   ‚Ä¢ ‚úÖ Model trained on 60,000 images\n",
      "\n",
      "üìä Evaluation on Training Set:\n",
      "   ‚Ä¢ Fit Duration: 0:00:00.508681\n",
      "   ‚Ä¢ ‚úÖ Model trained on 60,000 images\n",
      "\n",
      "üìä Evaluation on Training Set:\n",
      "   ‚Ä¢ Train Accuracy: 0.801583 (80.16%)\n",
      "   ‚Ä¢ Prediction Duration: 0:00:13.377813\n",
      "\n",
      "üìä Evaluation on Test Set:\n",
      "   ‚Ä¢ Train Accuracy: 0.801583 (80.16%)\n",
      "   ‚Ä¢ Prediction Duration: 0:00:13.377813\n",
      "\n",
      "üìä Evaluation on Test Set:\n",
      "   ‚Ä¢ Test Accuracy: 0.814100 (81.41%)\n",
      "   ‚Ä¢ Prediction Duration: 0:00:03.722223\n",
      "\n",
      "======================================================================\n",
      "üìà Summary:\n",
      "======================================================================\n",
      "‚úÖ Training Accuracy:  80.16% (48,095/60,000 correct)\n",
      "‚úÖ Test Accuracy:      81.41% (8,141/10,000 correct)\n",
      "\n",
      "‚è±Ô∏è Performance:\n",
      "   ‚Ä¢ Training Time: 0:00:00.508681\n",
      "   ‚Ä¢ Train Prediction: 0:00:13.377813\n",
      "   ‚Ä¢ Test Prediction: 0:00:03.722223\n",
      "\n",
      "üí° Analysis:\n",
      "======================================================================\n",
      "‚úÖ Train ‚âà Test = ‡πÑ‡∏°‡πà Overfit (Good Generalization)\n",
      "‚úÖ Accuracy > 80% = ‡∏î‡∏µ‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Naive Bayes!\n",
      "\n",
      "üéØ ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\n",
      "   ‚Ä¢ Gaussian NB ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏±‡∏ö MNIST (continuous features)\n",
      "   ‚Ä¢ ‡∏ñ‡∏∂‡∏á‡πÅ‡∏°‡πâ pixels ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞ ‡πÅ‡∏ï‡πà Naive assumption ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ\n",
      "   ‚Ä¢ Training ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å (‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì mean/variance)\n",
      "   ‚Ä¢ Prediction ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å (matrix operations)\n",
      "\n",
      "üìö ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö sklearn:\n",
      "   ‚Ä¢ sklearn GaussianNB ‡∏°‡∏µ accuracy ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô\n",
      "   ‚Ä¢ Implementation ‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏¢‡πÉ‡∏ô\n",
      "   ‚Ä¢ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
      "   ‚Ä¢ Test Accuracy: 0.814100 (81.41%)\n",
      "   ‚Ä¢ Prediction Duration: 0:00:03.722223\n",
      "\n",
      "======================================================================\n",
      "üìà Summary:\n",
      "======================================================================\n",
      "‚úÖ Training Accuracy:  80.16% (48,095/60,000 correct)\n",
      "‚úÖ Test Accuracy:      81.41% (8,141/10,000 correct)\n",
      "\n",
      "‚è±Ô∏è Performance:\n",
      "   ‚Ä¢ Training Time: 0:00:00.508681\n",
      "   ‚Ä¢ Train Prediction: 0:00:13.377813\n",
      "   ‚Ä¢ Test Prediction: 0:00:03.722223\n",
      "\n",
      "üí° Analysis:\n",
      "======================================================================\n",
      "‚úÖ Train ‚âà Test = ‡πÑ‡∏°‡πà Overfit (Good Generalization)\n",
      "‚úÖ Accuracy > 80% = ‡∏î‡∏µ‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Naive Bayes!\n",
      "\n",
      "üéØ ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\n",
      "   ‚Ä¢ Gaussian NB ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏±‡∏ö MNIST (continuous features)\n",
      "   ‚Ä¢ ‡∏ñ‡∏∂‡∏á‡πÅ‡∏°‡πâ pixels ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞ ‡πÅ‡∏ï‡πà Naive assumption ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ\n",
      "   ‚Ä¢ Training ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å (‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì mean/variance)\n",
      "   ‚Ä¢ Prediction ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å (matrix operations)\n",
      "\n",
      "üìö ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö sklearn:\n",
      "   ‚Ä¢ sklearn GaussianNB ‡∏°‡∏µ accuracy ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô\n",
      "   ‚Ä¢ Implementation ‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏¢‡πÉ‡∏ô\n",
      "   ‚Ä¢ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Training Gaussian Naive Bayes Model...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á model\n",
    "model = GaussianNB(smoothing=1e-2)\n",
    "print(\"‚úÖ Model Created with smoothing = 0.01\")\n",
    "\n",
    "# Training\n",
    "print(\"\\n‚è±Ô∏è Training Phase:\")\n",
    "t0 = datetime.now()\n",
    "model.fit(X_train, y_train)\n",
    "fit_duration = datetime.now() - t0\n",
    "print(f\"   ‚Ä¢ Fit Duration: {fit_duration}\")\n",
    "print(f\"   ‚Ä¢ ‚úÖ Model trained on {len(X_train):,} images\")\n",
    "\n",
    "# Evaluate on Training Set\n",
    "print(\"\\nüìä Evaluation on Training Set:\")\n",
    "t0 = datetime.now()\n",
    "train_acc = model.score(X_train, y_train)\n",
    "train_pred_duration = datetime.now() - t0\n",
    "print(f\"   ‚Ä¢ Train Accuracy: {train_acc:.6f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Prediction Duration: {train_pred_duration}\")\n",
    "\n",
    "# Evaluate on Test Set\n",
    "print(\"\\nüìä Evaluation on Test Set:\")\n",
    "t0 = datetime.now()\n",
    "test_acc = model.score(X_test, y_test)\n",
    "test_pred_duration = datetime.now() - t0\n",
    "print(f\"   ‚Ä¢ Test Accuracy: {test_acc:.6f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Prediction Duration: {test_pred_duration}\")\n",
    "\n",
    "# ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìà Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"‚úÖ Training Accuracy:  {train_acc*100:.2f}% ({int(train_acc*len(X_train)):,}/{len(X_train):,} correct)\")\n",
    "print(f\"‚úÖ Test Accuracy:      {test_acc*100:.2f}% ({int(test_acc*len(X_test)):,}/{len(X_test):,} correct)\")\n",
    "print(f\"\\n‚è±Ô∏è Performance:\")\n",
    "print(f\"   ‚Ä¢ Training Time: {fit_duration}\")\n",
    "print(f\"   ‚Ä¢ Train Prediction: {train_pred_duration}\")\n",
    "print(f\"   ‚Ä¢ Test Prediction: {test_pred_duration}\")\n",
    "\n",
    "# ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå\n",
    "print(\"\\nüí° Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "if abs(train_acc - test_acc) < 0.02:\n",
    "    print(\"‚úÖ Train ‚âà Test = ‡πÑ‡∏°‡πà Overfit (Good Generalization)\")\n",
    "elif train_acc > test_acc + 0.05:\n",
    "    print(\"‚ö†Ô∏è Train >> Test = Overfit (‡∏à‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)\")\n",
    "else:\n",
    "    print(\"‚úÖ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ\")\n",
    "\n",
    "if test_acc > 0.80:\n",
    "    print(\"‚úÖ Accuracy > 80% = ‡∏î‡∏µ‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Naive Bayes!\")\n",
    "elif test_acc > 0.70:\n",
    "    print(\"‚úÖ Accuracy > 70% = ‡∏î‡∏µ‡∏û‡∏≠‡πÉ‡∏ä‡πâ\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Accuracy ‡∏ï‡πà‡∏≥ = ‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á\")\n",
    "\n",
    "print(\"\\nüéØ ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "print(\"   ‚Ä¢ Gaussian NB ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏±‡∏ö MNIST (continuous features)\")\n",
    "print(\"   ‚Ä¢ ‡∏ñ‡∏∂‡∏á‡πÅ‡∏°‡πâ pixels ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞ ‡πÅ‡∏ï‡πà Naive assumption ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ\")\n",
    "print(\"   ‚Ä¢ Training ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å (‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì mean/variance)\")\n",
    "print(\"   ‚Ä¢ Prediction ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å (matrix operations)\")\n",
    "\n",
    "print(\"\\nüìö ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö sklearn:\")\n",
    "print(\"   ‚Ä¢ sklearn GaussianNB ‡∏°‡∏µ accuracy ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô\")\n",
    "print(\"   ‚Ä¢ Implementation ‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏¢‡πÉ‡∏ô\")\n",
    "print(\"   ‚Ä¢ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2fc8a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "\n",
    "### ‚úÖ Key Takeaways:\n",
    "\n",
    "1. **Implementation from Scratch**:\n",
    "   - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Gaussian NB ‡∏•‡∏∂‡∏Å‡∏ã‡∏∂‡πâ‡∏á\n",
    "   - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Prior, Mean, Variance ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ class\n",
    "   - ‡πÉ‡∏ä‡πâ log-space ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô numerical underflow\n",
    "\n",
    "2. **Gaussian NB Components**:\n",
    "   - **Prior**: $P(y) = \\frac{N_k}{N}$ (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ class)\n",
    "   - **Mean**: $\\mu_{k,d}$ (‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á feature ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö class k)\n",
    "   - **Variance**: $\\sigma_{k,d}^2$ (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á feature)\n",
    "\n",
    "3. **MNIST Performance**:\n",
    "   - Accuracy ~55-60% (‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Naive Bayes!)\n",
    "   - Training ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å (‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)\n",
    "   - Prediction ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å (real-time capable)\n",
    "\n",
    "4. **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ vs ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢**:\n",
    "   - ‚úÖ ‡πÄ‡∏£‡πá‡∏ß (Linear time complexity)\n",
    "   - ‚úÖ Simple (‡πÑ‡∏°‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô)\n",
    "   - ‚úÖ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö high-dimensional data (784 features)\n",
    "   - ‚ùå Naive assumption (pixels ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏à‡∏£‡∏¥‡∏á ‡πÜ)\n",
    "   - ‚ùå Accuracy ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ CNN, SVM\n",
    "\n",
    "5. **‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ**:\n",
    "   - **Smoothing**: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô variance = 0\n",
    "   - **Log-space**: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô underflow ‡∏à‡∏≤‡∏Å probability ‡πÄ‡∏•‡πá‡∏Å‡∏°‡∏≤‡∏Å\n",
    "   - **Multivariate Normal**: ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ features ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö Gaussian\n",
    "\n",
    "### üí° ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\n",
    "\n",
    "**‡∏ó‡∏≥‡πÑ‡∏° Gaussian NB ‡∏ñ‡∏∂‡∏á‡πÄ‡∏£‡πá‡∏ß?**\n",
    "- Training: ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì mean/variance (O(N√óD))\n",
    "- Prediction: matrix multiplication + argmax (O(N√óD√óK))\n",
    "- ‡πÑ‡∏°‡πà‡∏°‡∏µ iterative optimization (‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô Neural Networks)\n",
    "\n",
    "**‡∏ó‡∏≥‡πÑ‡∏° Accuracy ‡πÑ‡∏°‡πà‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å?**\n",
    "- **Naive Assumption**: Pixels ‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå (‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞)\n",
    "- **Simple Model**: ‡πÑ‡∏°‡πà‡∏à‡∏±‡∏ö complex patterns ‡πÑ‡∏î‡πâ\n",
    "- **Solution**: ‡πÉ‡∏ä‡πâ CNN, SVM, Random Forest ‡πÅ‡∏ó‡∏ô\n",
    "\n",
    "### üöÄ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ (Advanced):\n",
    "\n",
    "1. **Improve Performance**:\n",
    "   - Feature Engineering (PCA, feature selection)\n",
    "   - Ensemble Methods (combine with other models)\n",
    "   - Try different smoothing values\n",
    "\n",
    "2. **Try Other Datasets**:\n",
    "   - Fashion MNIST\n",
    "   - CIFAR-10 (color images)\n",
    "   - Custom datasets\n",
    "\n",
    "3. **Compare with Other Models**:\n",
    "   - sklearn's GaussianNB\n",
    "   - SVM\n",
    "   - Random Forest\n",
    "   - Neural Networks\n",
    "\n",
    "4. **Visualization**:\n",
    "   - Plot learned means (‡∏î‡∏π‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏≠‡∏∞‡πÑ‡∏£)\n",
    "   - Confusion Matrix (‡∏î‡∏π‡∏ß‡πà‡∏≤ class ‡πÑ‡∏´‡∏ô‡∏™‡∏±‡∏ö‡∏™‡∏ô)\n",
    "   - Misclassified examples (‡∏î‡∏π‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö ‡∏™‡∏£‡∏∏‡∏õ Mathematical Concepts:\n",
    "\n",
    "### Bayes' Theorem:\n",
    "$$P(y|X) = \\frac{P(X|y) \\times P(y)}{P(X)}$$\n",
    "\n",
    "### Gaussian Likelihood:\n",
    "$$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{yi}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{yi})^2}{2\\sigma_{yi}^2}\\right)$$\n",
    "\n",
    "### Log-Probability (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô code):\n",
    "$$\\log P(y|X) = \\sum_{i=1}^{D} \\log P(x_i|y) + \\log P(y)$$\n",
    "\n",
    "**‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ log?**\n",
    "- Probability ‡πÄ‡∏•‡πá‡∏Å‡∏°‡∏≤‡∏Å (< 1e-308) ‚Üí Underflow\n",
    "- Log ‡πÅ‡∏õ‡∏•‡∏á multiplication ‚Üí addition\n",
    "- Stable numerically ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06314b87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è Step 5: Train Model & Evaluate Performance\n",
    "\n",
    "### üöÄ Training + Testing Process\n",
    "\n",
    "**‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô**:\n",
    "1. ‡∏™‡∏£‡πâ‡∏≤‡∏á model (smoothing=1e-2)\n",
    "2. **Fit**: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 60,000 ‡∏†‡∏≤‡∏û\n",
    "3. **Evaluate**: ‡∏ß‡∏±‡∏î accuracy ‡∏ö‡∏ô train/test sets\n",
    "4. **Measure Speed**: ‡∏ß‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ fit ‡πÅ‡∏•‡∏∞ predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d884a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Step 4: Preprocess Data\n",
    "\n",
    "**‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:\n",
    "\n",
    "1. **Reshape**: (60000, 28, 28) ‚Üí (60000, 784)\n",
    "   - ‡πÅ‡∏õ‡∏•‡∏á‡∏†‡∏≤‡∏û 2D ‡πÄ‡∏õ‡πá‡∏ô vector 1D\n",
    "   - ‡πÅ‡∏ï‡πà‡∏•‡∏∞ pixel ‡πÄ‡∏õ‡πá‡∏ô 1 feature\n",
    "\n",
    "2. **Normalize**: [0, 255] ‚Üí [0, 1]\n",
    "   - ‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢ 255 ‡πÄ‡∏û‡∏∑‡πà‡∏≠ scale ‡∏Ñ‡πà‡∏≤ pixel\n",
    "   - ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
