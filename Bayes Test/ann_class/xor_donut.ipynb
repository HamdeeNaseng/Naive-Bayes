{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "812744af",
   "metadata": {},
   "source": [
    "# üß† XOR & Donut Problems: ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Neural Network?\n",
    "\n",
    "**‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå**: ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤ Neural Network ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà **Linear Models ‡πÅ‡∏Å‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ**\n",
    "\n",
    "**‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö**: `machine_learning_examples-master/ann_class/xor_donut.py`\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Neural Network?\n",
    "\n",
    "### **Linear vs Non-Linear Problems**:\n",
    "\n",
    "| Problem | Can Linear Model Solve? | Need Neural Network? |\n",
    "|---------|------------------------|---------------------|\n",
    "| **AND** | ‚úÖ Yes | ‚ùå No |\n",
    "| **OR** | ‚úÖ Yes | ‚ùå No |\n",
    "| **XOR** | ‚ùå **No!** | ‚úÖ **Yes!** |\n",
    "| **Donut** | ‚ùå **No!** | ‚úÖ **Yes!** |\n",
    "\n",
    "üí° **XOR ‡πÅ‡∏•‡∏∞ Donut** = ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ **Non-Linear** ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á‡πÑ‡∏î‡πâ\n",
    "\n",
    "---\n",
    "\n",
    "## üî¥ Problem 1: XOR (Exclusive OR)\n",
    "\n",
    "### **XOR Truth Table**:\n",
    "\n",
    "| X‚ÇÄ | X‚ÇÅ | Output |\n",
    "|----|----|--------|\n",
    "| 0  | 0  | **0** |\n",
    "| 0  | 1  | **1** |\n",
    "| 1  | 0  | **1** |\n",
    "| 1  | 1  | **0** |\n",
    "\n",
    "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤**: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á 1 ‡πÄ‡∏™‡πâ‡∏ô‡πÑ‡∏î‡πâ!\n",
    "\n",
    "**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ**: ‡πÉ‡∏ä‡πâ Neural Network ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ **non-linear decision boundary**\n",
    "\n",
    "---\n",
    "\n",
    "## üç© Problem 2: Donut (Concentric Circles)\n",
    "\n",
    "**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:\n",
    "- **Inner Circle** (radius ~5) ‚Üí Class 0\n",
    "- **Outer Circle** (radius ~10) ‚Üí Class 1\n",
    "\n",
    "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤**: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á‡πÑ‡∏î‡πâ! (‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ß‡∏á‡∏Å‡∏•‡∏°)\n",
    "\n",
    "**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ**: Neural Network ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ circular decision boundary\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Architecture (Binary Classification)\n",
    "\n",
    "```\n",
    "Input (2D) ‚Üí Hidden (M neurons, ReLU) ‚Üí Output (1 neuron, Sigmoid)\n",
    "```\n",
    "\n",
    "**Key Differences from Multi-Class**:\n",
    "- Output: **1 neuron** (not K)\n",
    "- Activation: **Sigmoid** (not Softmax)\n",
    "- Loss: **Binary Cross-Entropy** (not Categorical)\n",
    "\n",
    "$$\\text{Loss} = -[T \\log(Y) + (1-T) \\log(1-Y)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9646fb",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdadd518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡πÑ‡∏ó‡∏¢\n",
    "font_path = '../../font/Prompt/Prompt-Regular.ttf'\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "fm.fontManager.addfont(font_path)\n",
    "plt.rcParams['font.family'] = font_prop.get_name()\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ‡πÄ‡∏û‡∏¥‡πà‡∏° path\n",
    "sys.path.append('../../machine_learning_examples-master')\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ XOR & Donut Problems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0eaf1",
   "metadata": {},
   "source": [
    "## üîß Step 2: Helper Functions\n",
    "\n",
    "### 2.1 Forward Propagation (Binary Classification)\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "X ‚Üí Z = ReLU(X¬∑W1 + b1) ‚Üí Y = Sigmoid(Z¬∑W2 + b2)\n",
    "```\n",
    "\n",
    "**Key**: Output is **single value** (not vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c93312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Forward Propagation for Binary Classification\n",
    "    \n",
    "    Architecture:\n",
    "    Input ‚Üí Hidden (ReLU) ‚Üí Output (Sigmoid)\n",
    "    \n",
    "    Parameters:\n",
    "    - X: input (N, 2)\n",
    "    - W1, b1: hidden layer weights\n",
    "    - W2, b2: output layer weights\n",
    "    \n",
    "    Returns:\n",
    "    - Y: predictions (N,) - probabilities [0, 1]\n",
    "    - Z: hidden activations (N, M)\n",
    "    \"\"\"\n",
    "    # Hidden Layer: Z = ReLU(X¬∑W1 + b1)\n",
    "    Z = X.dot(W1) + b1\n",
    "    Z = Z * (Z > 0)  # ReLU: max(0, x)\n",
    "    \n",
    "    # Output Layer: Y = Sigmoid(Z¬∑W2 + b2)\n",
    "    activation = Z.dot(W2) + b2\n",
    "    Y = 1 / (1 + np.exp(-activation))  # Sigmoid\n",
    "    \n",
    "    return Y, Z\n",
    "\n",
    "def predict(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Predict class labels (0 or 1)\n",
    "    \"\"\"\n",
    "    Y, _ = forward(X, W1, b1, W2, b2)\n",
    "    return np.round(Y)  # 0.5 threshold\n",
    "\n",
    "print(\"‚úÖ Forward & Predict functions ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß!\")\n",
    "print(\"\")\n",
    "print(\"üí° Binary Classification:\")\n",
    "print(\"   ‚Ä¢ Output = 1 neuron (Sigmoid) ‚Üí probability [0, 1]\")\n",
    "print(\"   ‚Ä¢ Prediction = round(Y) ‚Üí 0 or 1\")\n",
    "print(\"   ‚Ä¢ ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å Multi-class (Softmax + argmax)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44600bf",
   "metadata": {},
   "source": [
    "### 2.2 Gradient Functions (Backpropagation)\n",
    "\n",
    "**Binary Cross-Entropy Gradients**:\n",
    "- Output layer: $(T - Y) \\cdot Z$\n",
    "- Hidden layer: $(T - Y) \\cdot W_2^T \\odot \\mathbb{1}_{Z>0}$ (ReLU derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8295b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_w2(Z, T, Y):\n",
    "    \"\"\"\n",
    "    Gradient ‡∏Ç‡∏≠‡∏á W2 (output weights)\n",
    "    \"\"\"\n",
    "    return (T - Y).dot(Z)\n",
    "\n",
    "def derivative_b2(T, Y):\n",
    "    \"\"\"\n",
    "    Gradient ‡∏Ç‡∏≠‡∏á b2 (output bias)\n",
    "    \"\"\"\n",
    "    return (T - Y).sum()\n",
    "\n",
    "def derivative_w1(X, Z, T, Y, W2):\n",
    "    \"\"\"\n",
    "    Gradient ‡∏Ç‡∏≠‡∏á W1 (hidden weights)\n",
    "    \n",
    "    Note: ReLU derivative = 1 if Z > 0, else 0\n",
    "    \"\"\"\n",
    "    dZ = np.outer(T - Y, W2) * (Z > 0)  # ReLU derivative\n",
    "    return X.T.dot(dZ)\n",
    "\n",
    "def derivative_b1(Z, T, Y, W2):\n",
    "    \"\"\"\n",
    "    Gradient ‡∏Ç‡∏≠‡∏á b1 (hidden bias)\n",
    "    \"\"\"\n",
    "    dZ = np.outer(T - Y, W2) * (Z > 0)  # ReLU derivative\n",
    "    return dZ.sum(axis=0)\n",
    "\n",
    "print(\"‚úÖ Gradient functions ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb70422",
   "metadata": {},
   "source": [
    "### 2.3 Loss Function (Binary Cross-Entropy)\n",
    "\n",
    "**Log-Likelihood** (‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á = ‡∏î‡∏µ):\n",
    "\n",
    "$$\\text{LL} = \\sum [T \\log(Y) + (1-T) \\log(1-Y)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_likelihood(T, Y):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy (as Log-Likelihood)\n",
    "    \n",
    "    LL = Œ£[T¬∑log(Y) + (1-T)¬∑log(1-Y)]\n",
    "    \n",
    "    Higher = Better!\n",
    "    \"\"\"\n",
    "    return np.sum(T * np.log(Y) + (1 - T) * np.log(1 - Y))\n",
    "\n",
    "print(\"‚úÖ Loss function ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß!\")\n",
    "print(\"\")\n",
    "print(\"üí° Binary Cross-Entropy:\")\n",
    "print(\"   ‚Ä¢ T = 1: Loss = -log(Y) ‚Üí ‡∏¢‡∏¥‡πà‡∏á Y ‚Üí 1 ‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ\")\n",
    "print(\"   ‚Ä¢ T = 0: Loss = -log(1-Y) ‚Üí ‡∏¢‡∏¥‡πà‡∏á Y ‚Üí 0 ‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ\")\n",
    "print(\"   ‚Ä¢ Log-Likelihood ‡∏™‡∏π‡∏á = Loss ‡∏ï‡πà‡∏≥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cff8c7",
   "metadata": {},
   "source": [
    "## üî¥ Problem 1: XOR (Exclusive OR)\n",
    "\n",
    "### Step 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• XOR\n",
    "\n",
    "**XOR Logic**:\n",
    "- (0, 0) ‚Üí 0\n",
    "- (0, 1) ‚Üí 1\n",
    "- (1, 0) ‚Üí 1\n",
    "- (1, 1) ‚Üí 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR Data\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "print(\"‚úÖ XOR Data:\")\n",
    "print(\"\")\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"   Input: {X_xor[i]} ‚Üí Output: {Y_xor[i]}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red' if y == 0 else 'blue' for y in Y_xor]\n",
    "plt.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=500, edgecolors='black', linewidth=3)\n",
    "plt.title('üî¥ XOR Problem: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á‡πÑ‡∏î‡πâ!', fontsize=13, fontweight='bold', pad=15)\n",
    "plt.xlabel('X‚ÇÄ', fontsize=11)\n",
    "plt.ylabel('X‚ÇÅ', fontsize=11)\n",
    "plt.xticks([0, 1])\n",
    "plt.yticks([0, 1])\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add labels\n",
    "for i, txt in enumerate(Y_xor):\n",
    "    plt.text(X_xor[i, 0], X_xor[i, 1], f'  {txt}', fontsize=20, fontweight='bold')\n",
    "\n",
    "plt.legend(['Class 0 (Red)', 'Class 1 (Blue)'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\")\n",
    "print(\"üí° ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Neural Network?\")\n",
    "print(\"   ‚Ä¢ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á 1 ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å Red ‡πÅ‡∏•‡∏∞ Blue ‡πÑ‡∏î‡πâ\")\n",
    "print(\"   ‚Ä¢ ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Non-linear transformation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7bf5f2",
   "metadata": {},
   "source": [
    "### Step 4: Train Neural Network (XOR)\n",
    "\n",
    "**Architecture**: Input (2) ‚Üí Hidden (5, ReLU) ‚Üí Output (1, Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0fd7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "W1_xor = np.random.randn(2, 5)\n",
    "b1_xor = np.zeros(5)\n",
    "W2_xor = np.random.randn(5)\n",
    "b2_xor = 0\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 1e-2\n",
    "regularization = 0.0\n",
    "epochs = 30000\n",
    "\n",
    "LL_xor = []  # Track log-likelihoods\n",
    "accuracies_xor = []  # Track accuracies\n",
    "\n",
    "print(\"üèãÔ∏è Training Neural Network ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö XOR...\\n\")\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Forward pass\n",
    "    pY, Z = forward(X_xor, W1_xor, b1_xor, W2_xor, b2_xor)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ll = get_log_likelihood(Y_xor, pY)\n",
    "    prediction = predict(X_xor, W1_xor, b1_xor, W2_xor, b2_xor)\n",
    "    accuracy = np.mean(prediction == Y_xor)\n",
    "    \n",
    "    LL_xor.append(ll)\n",
    "    accuracies_xor.append(accuracy)\n",
    "    \n",
    "    # Backpropagation\n",
    "    gW2 = derivative_w2(Z, Y_xor, pY)\n",
    "    gb2 = derivative_b2(Y_xor, pY)\n",
    "    gW1 = derivative_w1(X_xor, Z, Y_xor, pY, W2_xor)\n",
    "    gb1 = derivative_b1(Z, Y_xor, pY, W2_xor)\n",
    "    \n",
    "    # Update weights (Gradient Ascent for Log-Likelihood)\n",
    "    W2_xor += learning_rate * (gW2 - regularization * W2_xor)\n",
    "    b2_xor += learning_rate * (gb2 - regularization * b2_xor)\n",
    "    W1_xor += learning_rate * (gW1 - regularization * W1_xor)\n",
    "    b1_xor += learning_rate * (gb1 - regularization * b1_xor)\n",
    "    \n",
    "    # Print progress\n",
    "    if i % 5000 == 0:\n",
    "        print(f\"Epoch {i:5d} | LL: {ll:8.4f} | Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!\")\n",
    "print(f\"\")\n",
    "print(f\"üìä Final Results:\")\n",
    "print(f\"   ‚Ä¢ Log-Likelihood: {LL_xor[-1]:.4f}\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {accuracies_xor[-1]:.4f} ({accuracies_xor[-1]*100:.1f}%)\")\n",
    "print(f\"\")\n",
    "print(f\"üéØ Predictions:\")\n",
    "final_pred_xor = predict(X_xor, W1_xor, b1_xor, W2_xor, b2_xor)\n",
    "for i in range(len(X_xor)):\n",
    "    status = \"‚úÖ\" if final_pred_xor[i] == Y_xor[i] else \"‚ùå\"\n",
    "    print(f\"   {X_xor[i]} ‚Üí True: {Y_xor[i]}, Pred: {int(final_pred_xor[i])} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot XOR Training Progress\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Log-Likelihood\n",
    "ax[0].plot(LL_xor, linewidth=2, color='blue')\n",
    "ax[0].set_xlabel('Epoch', fontsize=11)\n",
    "ax[0].set_ylabel('Log-Likelihood', fontsize=11)\n",
    "ax[0].set_title('üìà Log-Likelihood ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô (‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)', fontsize=12, fontweight='bold')\n",
    "ax[0].grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# Plot 2: Accuracy\n",
    "ax[1].plot(accuracies_xor, linewidth=2, color='green')\n",
    "ax[1].set_xlabel('Epoch', fontsize=11)\n",
    "ax[1].set_ylabel('Accuracy', fontsize=11)\n",
    "ax[1].set_title('üéØ Accuracy ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô', fontsize=12, fontweight='bold')\n",
    "ax[1].set_ylim([0, 1.1])\n",
    "ax[1].grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.suptitle('üî¥ XOR Problem: Neural Network ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Neural Network ‡πÅ‡∏Å‡πâ XOR ‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß!\")\n",
    "print(\"   ‚Ä¢ Accuracy = 100% ‚Üí ‡∏ó‡∏∏‡∏Å point ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\")\n",
    "print(\"   ‚Ä¢ ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ non-linear decision boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650eb0f9",
   "metadata": {},
   "source": [
    "## üç© Problem 2: Donut (Concentric Circles)\n",
    "\n",
    "### Step 5: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Donut\n",
    "\n",
    "**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:\n",
    "- Inner circle (radius ‚âà 5) ‚Üí Class 0\n",
    "- Outer circle (radius ‚âà 10) ‚Üí Class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbefabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donut Data Parameters\n",
    "N = 1000\n",
    "R_inner = 5\n",
    "R_outer = 10\n",
    "\n",
    "# Inner Circle (Class 0)\n",
    "R1 = np.random.randn(N//2) + R_inner\n",
    "theta1 = 2 * np.pi * np.random.random(N//2)\n",
    "X_inner = np.concatenate([[R1 * np.cos(theta1)], [R1 * np.sin(theta1)]]).T\n",
    "\n",
    "# Outer Circle (Class 1)\n",
    "R2 = np.random.randn(N//2) + R_outer\n",
    "theta2 = 2 * np.pi * np.random.random(N//2)\n",
    "X_outer = np.concatenate([[R2 * np.cos(theta2)], [R2 * np.sin(theta2)]]).T\n",
    "\n",
    "# Combine\n",
    "X_donut = np.concatenate([X_inner, X_outer])\n",
    "Y_donut = np.array([0] * (N//2) + [1] * (N//2))\n",
    "\n",
    "print(f\"‚úÖ Donut Data ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!\")\n",
    "print(f\"   ‚Ä¢ Inner circle: {N//2} samples (Class 0)\")\n",
    "print(f\"   ‚Ä¢ Outer circle: {N//2} samples (Class 1)\")\n",
    "print(f\"   ‚Ä¢ Total: {N} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Donut\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_donut[:, 0], X_donut[:, 1], c=Y_donut, s=50, alpha=0.6, \n",
    "            cmap='coolwarm', edgecolors='black', linewidth=0.5)\n",
    "plt.title('üç© Donut Problem: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á‡πÑ‡∏î‡πâ!', fontsize=13, fontweight='bold', pad=15)\n",
    "plt.xlabel('X‚ÇÄ', fontsize=11)\n",
    "plt.ylabel('X‚ÇÅ', fontsize=11)\n",
    "plt.colorbar(label='Class')\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\")\n",
    "print(\"üí° ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Neural Network?\")\n",
    "print(\"   ‚Ä¢ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å Inner ‡πÅ‡∏•‡∏∞ Outer circle ‡πÑ‡∏î‡πâ\")\n",
    "print(\"   ‚Ä¢ ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ circular decision boundary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7baf758",
   "metadata": {},
   "source": [
    "### Step 6: Train Neural Network (Donut)\n",
    "\n",
    "**Architecture**: Input (2) ‚Üí Hidden (8, ReLU) ‚Üí Output (1, Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06385f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "n_hidden = 8\n",
    "W1_donut = np.random.randn(2, n_hidden)\n",
    "b1_donut = np.random.randn(n_hidden)\n",
    "W2_donut = np.random.randn(n_hidden)\n",
    "b2_donut = np.random.randn(1)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.00005\n",
    "regularization = 0.2\n",
    "epochs = 3000\n",
    "\n",
    "LL_donut = []  # Track log-likelihoods\n",
    "accuracies_donut = []  # Track accuracies\n",
    "\n",
    "print(\"üèãÔ∏è Training Neural Network ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Donut...\\n\")\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Forward pass\n",
    "    pY, Z = forward(X_donut, W1_donut, b1_donut, W2_donut, b2_donut)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ll = get_log_likelihood(Y_donut, pY)\n",
    "    prediction = predict(X_donut, W1_donut, b1_donut, W2_donut, b2_donut)\n",
    "    accuracy = 1 - np.abs(prediction - Y_donut).mean()\n",
    "    \n",
    "    LL_donut.append(ll)\n",
    "    accuracies_donut.append(accuracy)\n",
    "    \n",
    "    # Backpropagation\n",
    "    gW2 = derivative_w2(Z, Y_donut, pY)\n",
    "    gb2 = derivative_b2(Y_donut, pY)\n",
    "    gW1 = derivative_w1(X_donut, Z, Y_donut, pY, W2_donut)\n",
    "    gb1 = derivative_b1(Z, Y_donut, pY, W2_donut)\n",
    "    \n",
    "    # Update weights (Gradient Ascent + L2 Regularization)\n",
    "    W2_donut += learning_rate * (gW2 - regularization * W2_donut)\n",
    "    b2_donut += learning_rate * (gb2 - regularization * b2_donut)\n",
    "    W1_donut += learning_rate * (gW1 - regularization * W1_donut)\n",
    "    b1_donut += learning_rate * (gb1 - regularization * b1_donut)\n",
    "    \n",
    "    # Print progress\n",
    "    if i % 300 == 0:\n",
    "        print(f\"Epoch {i:4d} | LL: {ll:10.2f} | Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!\")\n",
    "print(f\"\")\n",
    "print(f\"üìä Final Results:\")\n",
    "print(f\"   ‚Ä¢ Log-Likelihood: {LL_donut[-1]:.2f}\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {accuracies_donut[-1]:.4f} ({accuracies_donut[-1]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Donut Training Progress\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Log-Likelihood\n",
    "ax[0].plot(LL_donut, linewidth=2, color='blue')\n",
    "ax[0].set_xlabel('Epoch', fontsize=11)\n",
    "ax[0].set_ylabel('Log-Likelihood', fontsize=11)\n",
    "ax[0].set_title('üìà Log-Likelihood ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô (‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)', fontsize=12, fontweight='bold')\n",
    "ax[0].grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# Plot 2: Accuracy\n",
    "ax[1].plot(accuracies_donut, linewidth=2, color='green')\n",
    "ax[1].set_xlabel('Epoch', fontsize=11)\n",
    "ax[1].set_ylabel('Accuracy', fontsize=11)\n",
    "ax[1].set_title('üéØ Accuracy ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô', fontsize=12, fontweight='bold')\n",
    "ax[1].set_ylim([0, 1.1])\n",
    "ax[1].grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.suptitle('üç© Donut Problem: Neural Network ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c8df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Predictions\n",
    "final_pred_donut = predict(X_donut, W1_donut, b1_donut, W2_donut, b2_donut)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Plot 1: True Labels\n",
    "ax[0].scatter(X_donut[:, 0], X_donut[:, 1], c=Y_donut, s=50, alpha=0.6,\n",
    "              cmap='coolwarm', edgecolors='black', linewidth=0.5)\n",
    "ax[0].set_title('‚úÖ True Labels', fontsize=12, fontweight='bold')\n",
    "ax[0].set_xlabel('X‚ÇÄ', fontsize=11)\n",
    "ax[0].set_ylabel('X‚ÇÅ', fontsize=11)\n",
    "ax[0].grid(alpha=0.3, linestyle='--')\n",
    "ax[0].axis('equal')\n",
    "\n",
    "# Plot 2: Predicted Labels\n",
    "scatter = ax[1].scatter(X_donut[:, 0], X_donut[:, 1], c=final_pred_donut, s=50, alpha=0.6,\n",
    "                        cmap='coolwarm', edgecolors='black', linewidth=0.5)\n",
    "ax[1].set_title(f'üîÆ Predicted Labels (Accuracy: {accuracies_donut[-1]*100:.1f}%)', \n",
    "                fontsize=12, fontweight='bold')\n",
    "ax[1].set_xlabel('X‚ÇÄ', fontsize=11)\n",
    "ax[1].set_ylabel('X‚ÇÅ', fontsize=11)\n",
    "ax[1].grid(alpha=0.3, linestyle='--')\n",
    "ax[1].axis('equal')\n",
    "\n",
    "plt.colorbar(scatter, ax=ax[1], label='Class')\n",
    "plt.suptitle('üéØ Donut: True vs Predicted', fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Neural Network ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ circular boundary ‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß!\")\n",
    "print(\"   ‚Ä¢ ‡πÅ‡∏¢‡∏Å Inner ‡πÅ‡∏•‡∏∞ Outer circle ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥\")\n",
    "print(\"   ‚Ä¢ Decision boundary ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏á‡∏Å‡∏•‡∏° (non-linear)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af85405",
   "metadata": {},
   "source": [
    "## üéì ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: XOR & Donut Problems\n",
    "\n",
    "### üîë Key Takeaways:\n",
    "\n",
    "#### 1Ô∏è‚É£ **‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Neural Network?**\n",
    "\n",
    "**Linear Models ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠**:\n",
    "- Logistic Regression = ‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á 1 ‡πÄ‡∏™‡πâ‡∏ô\n",
    "- XOR + Donut = **Non-linear** problems\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Neural Network ‡πÄ‡∏û‡∏∑‡πà‡∏≠ learn **non-linear transformations**\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ **XOR Problem**:\n",
    "\n",
    "| Input | Output | ‡∏ó‡∏≥‡πÑ‡∏°‡∏¢‡∏≤‡∏Å? |\n",
    "|-------|--------|----------|\n",
    "| (0,0) | 0 | ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á‡πÅ‡∏¢‡∏Å |\n",
    "| (0,1) | 1 | 4 points ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ |\n",
    "| (1,0) | 1 | ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ hidden layer |\n",
    "| (1,1) | 0 | ‡πÄ‡∏û‡∏∑‡πà‡∏≠ transform space |\n",
    "\n",
    "**Solution**:\n",
    "- Hidden layer ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ features ‡πÉ‡∏´‡∏°‡πà\n",
    "- Transform input space ‚Üí linearly separable\n",
    "- Output layer ‡πÉ‡∏ä‡πâ‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á‡πÅ‡∏¢‡∏Å‡πÉ‡∏ô new space\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ **Donut Problem**:\n",
    "\n",
    "**Challenge**:\n",
    "- Inner circle (Class 0) ‡∏•‡πâ‡∏≠‡∏°‡∏£‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ Outer circle (Class 1)\n",
    "- ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á‡πÅ‡∏¢‡∏Å‡πÑ‡∏î‡πâ\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ circular decision boundary\n",
    "\n",
    "**Solution**:\n",
    "- Neural Network ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ circular patterns\n",
    "- Hidden neurons capture different aspects of circle\n",
    "- Combine ‚Üí create circular boundary\n",
    "\n",
    "---\n",
    "\n",
    "#### 4Ô∏è‚É£ **Binary Classification Architecture**:\n",
    "\n",
    "```\n",
    "Input (2D) ‚Üí Hidden (M, ReLU) ‚Üí Output (1, Sigmoid)\n",
    "```\n",
    "\n",
    "**Key Differences from Multi-Class**:\n",
    "\n",
    "| | Binary | Multi-Class |\n",
    "|---|--------|-------------|\n",
    "| **Output Neurons** | 1 | K |\n",
    "| **Activation** | Sigmoid | Softmax |\n",
    "| **Output Range** | [0, 1] | [0, 1]·¥∑ (sum=1) |\n",
    "| **Loss** | Binary Cross-Entropy | Categorical Cross-Entropy |\n",
    "| **Prediction** | round(Y) | argmax(Y) |\n",
    "\n",
    "---\n",
    "\n",
    "### üí° ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "\n",
    "#### ‚úÖ **Non-Linear Problems**:\n",
    "- XOR ‡πÅ‡∏•‡∏∞ Donut = ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà Linear Models ‡πÅ‡∏Å‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\n",
    "- Neural Network ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ non-linear transformations\n",
    "- Hidden layers = feature learners\n",
    "\n",
    "#### ‚úÖ **Binary Classification**:\n",
    "- Output = 1 neuron with Sigmoid\n",
    "- Loss = Binary Cross-Entropy (Log-Likelihood)\n",
    "- Prediction = round(Y) ‚Üí 0 or 1\n",
    "\n",
    "#### ‚úÖ **Training Process**:\n",
    "- Forward Pass ‚Üí Calculate Loss ‚Üí Backpropagation ‚Üí Update Weights\n",
    "- Gradient Ascent ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Log-Likelihood (‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)\n",
    "- Regularization ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô overfitting\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ:\n",
    "\n",
    "#### 1Ô∏è‚É£ **‡∏•‡∏≠‡∏á Hidden Layers ‡∏≠‡∏∑‡πà‡∏ô**:\n",
    "```python\n",
    "# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô neurons\n",
    "W1 = np.random.randn(2, 10)  # 10 hidden neurons\n",
    "W1 = np.random.randn(2, 20)  # 20 hidden neurons\n",
    "\n",
    "# ‡∏•‡∏≠‡∏á deeper networks\n",
    "# Input ‚Üí Hidden1 ‚Üí Hidden2 ‚Üí Output\n",
    "```\n",
    "\n",
    "#### 2Ô∏è‚É£ **‡∏•‡∏≠‡∏á Activation ‡∏≠‡∏∑‡πà‡∏ô**:\n",
    "```python\n",
    "# ‡πÅ‡∏ó‡∏ô ReLU ‡∏î‡πâ‡∏ß‡∏¢:\n",
    "Z = np.tanh(X.dot(W1) + b1)              # Tanh\n",
    "Z = 1 / (1 + np.exp(-(X.dot(W1) + b1)))  # Sigmoid\n",
    "```\n",
    "\n",
    "#### 3Ô∏è‚É£ **‡∏•‡∏≠‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏≠‡∏∑‡πà‡∏ô**:\n",
    "- **Spiral**: 2 spirals intertwined\n",
    "- **Checkerboard**: alternating squares\n",
    "- **Moon**: two half-moon shapes\n",
    "\n",
    "#### 4Ô∏è‚É£ **Visualize Decision Boundary**:\n",
    "```python\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á mesh grid\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\n",
    "Z_grid = predict(X_grid, W1, b1, W2, b2)\n",
    "Z_grid = Z_grid.reshape(xx.shape)\n",
    "\n",
    "# Plot contour\n",
    "plt.contourf(xx, yy, Z_grid, alpha=0.3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìö ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Notebooks ‡∏≠‡∏∑‡πà‡∏ô:\n",
    "\n",
    "| Notebook | Problem Type | Output | Loss |\n",
    "|----------|--------------|--------|------|\n",
    "| **forwardprop** | Multi-class | K neurons (Softmax) | Categorical CE |\n",
    "| **backprop** | Multi-class | K neurons (Softmax) | Categorical CE |\n",
    "| **xor_donut** (‡∏ô‡∏µ‡πà!) | **Binary** | **1 neuron (Sigmoid)** | **Binary CE** |\n",
    "| **regression** | Regression | 1 neuron (Linear) | MSE |\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô‡πÜ:\n",
    "\n",
    "> **\"Neural Networks ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ Non-Linear Patterns ‡πÑ‡∏î‡πâ!\"**\n",
    "\n",
    "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ**:\n",
    "1. **XOR** - Logic gate ‡∏ó‡∏µ‡πà Linear Model ‡πÅ‡∏Å‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\n",
    "2. **Donut** - Circular patterns (concentric circles)\n",
    "\n",
    "**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ**:\n",
    "- Hidden layers ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ **non-linear transformations**\n",
    "- Transform input space ‚Üí linearly separable\n",
    "- Output layer ‡πÉ‡∏ä‡πâ‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á‡πÅ‡∏¢‡∏Å‡πÉ‡∏ô new space\n",
    "\n",
    "**Binary Classification**:\n",
    "- Output: 1 neuron (Sigmoid) ‚Üí probability [0, 1]\n",
    "- Loss: Binary Cross-Entropy (Log-Likelihood)\n",
    "- Prediction: round(Y) ‚Üí 0 or 1\n",
    "\n",
    "**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå**:\n",
    "- ‚úÖ XOR: Accuracy = 100% (4/4 correct)\n",
    "- ‚úÖ Donut: Accuracy ~95-99%\n",
    "- ‚úÖ Neural Network ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ non-linear boundaries ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n",
    "\n",
    "---\n",
    "\n",
    "### ü§î ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏¢‡∏ö‡∏ó:\n",
    "\n",
    "1. **‡∏ó‡∏≥‡πÑ‡∏° XOR ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Neural Network?**\n",
    "   - ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á 1 ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å 4 points ‡πÑ‡∏î‡πâ\n",
    "   - Hidden layer transform space ‚Üí linearly separable\n",
    "\n",
    "2. **Binary vs Multi-Class Classification?**\n",
    "   - Binary: 1 output (Sigmoid), threshold at 0.5\n",
    "   - Multi-Class: K outputs (Softmax), argmax\n",
    "\n",
    "3. **‡∏ó‡∏≥‡πÑ‡∏°‡πÉ‡∏ä‡πâ Log-Likelihood ‡πÅ‡∏ó‡∏ô Loss?**\n",
    "   - Log-Likelihood = -Loss (negative)\n",
    "   - Maximize LL = Minimize Loss\n",
    "   - ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô ‡πÅ‡∏Ñ‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô\n",
    "\n",
    "4. **Hidden layers ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£?**\n",
    "   - ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ features ‡πÉ‡∏´‡∏°‡πà (automatic feature engineering)\n",
    "   - Transform input space\n",
    "   - ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ non-linear ‚Üí linear\n",
    "\n",
    "‚ú® **Happy Learning with Non-Linear Problems!** ‚ú®"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
