{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå dna.csv ‡∏à‡∏≤‡∏Å‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "url = 'https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv'\n",
    "filename = 'bbc_text_cls.csv'\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
    "if not os.path.exists(filename):\n",
    "    print(f\"‚è¨ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î {filename}...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    print(f\"‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î {filename} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "else:\n",
    "    print(f\"‚úÖ ‡πÑ‡∏ü‡∏•‡πå {filename} ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a9892",
   "metadata": {},
   "source": [
    "# üì∞ BBC News Text Classification with Multinomial Naive Bayes\n",
    "\n",
    "**‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå**: ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ç‡πà‡∏≤‡∏ß BBC ‡∏ï‡∏≤‡∏°‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏î‡πâ‡∏ß‡∏¢ Multinomial Naive Bayes\n",
    "\n",
    "---\n",
    "\n",
    "## üìã ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå:\n",
    "\n",
    "### üéØ **‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢**:\n",
    "‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß BBC ‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß (Text Classification)\n",
    "\n",
    "### üì∞ **Dataset**: BBC News Articles\n",
    "- **‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**: ~2,225 ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß\n",
    "- **Features**: Text (‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß)\n",
    "- **Target**: Labels (‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏Ç‡πà‡∏≤‡∏ß)\n",
    "  - business (‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à)\n",
    "  - entertainment (‡∏ö‡∏±‡∏ô‡πÄ‡∏ó‡∏¥‡∏á)\n",
    "  - politics (‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á)\n",
    "  - sport (‡∏Å‡∏µ‡∏¨‡∏≤)\n",
    "  - tech (‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ)\n",
    "\n",
    "### üî¨ **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢**:\n",
    "- **Text Data**: ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á text ‡πÄ‡∏õ‡πá‡∏ô numbers\n",
    "- **High Dimensionality**: ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏´‡∏•‡∏≤‡∏¢‡∏û‡∏±‡∏ô‡∏Ñ‡∏≥\n",
    "- **Sparse Matrix**: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏≤‡∏ö‡∏≤‡∏á (‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏õ‡πá‡∏ô 0)\n",
    "\n",
    "---\n",
    "\n",
    "## üó∫Ô∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:\n",
    "\n",
    "1. **Download Data** üì•\n",
    "2. **Load & Explore** üîç (‡∏î‡∏π‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)\n",
    "3. **Vectorization** üî§ (‡πÅ‡∏õ‡∏•‡∏á text ‚Üí numbers)\n",
    "4. **Train Multinomial NB** üèãÔ∏è\n",
    "5. **Evaluate** üéØ (Accuracy, Confusion Matrix)\n",
    "6. **Analyze** üî¨ (Top words per class)\n",
    "\n",
    "---\n",
    "\n",
    "## üí° ‡∏ó‡∏≥‡πÑ‡∏°‡πÉ‡∏ä‡πâ **Multinomial NB**?\n",
    "\n",
    "**Multinomial Naive Bayes**:\n",
    "- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö **Count Data** (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏õ‡∏£‡∏≤‡∏Å‡∏è)\n",
    "- ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Text Classification, Document Classification\n",
    "- ‡πÄ‡∏£‡πá‡∏ß + ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• text\n",
    "\n",
    "**Gaussian NB**:\n",
    "- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Continuous features (‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö word counts)\n",
    "\n",
    "**Bernoulli NB**:\n",
    "- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Binary features (0/1 - ‡∏Ñ‡∏≥‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà)\n",
    "\n",
    "**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß BBC ‡∏ô‡∏µ‡πâ**:\n",
    "- Features ‡∏Ñ‡∏∑‡∏≠ word counts (0, 1, 2, 3, ...) ‚Üí ‡πÉ‡∏ä‡πâ **Multinomial NB** ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fc5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ font ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "font_path = '../../font/Prompt/Prompt-Regular.ttf'\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "fm.fontManager.addfont(font_path)\n",
    "font_name = font_prop.get_name()\n",
    "plt.rcParams['font.family'] = font_name\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Font ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô '{font_name}' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e8ff7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• Step 1: Download Dataset\n",
    "\n",
    "‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå BBC News Dataset ‡∏à‡∏≤‡∏Å Lazy Programmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd1005",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename)\n",
    "\n",
    "print(\"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å:\")\n",
    "print(\"=\" * 80)\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nüí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\")\n",
    "print(\"   ‚Ä¢ text: ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ï‡πá‡∏°)\")\n",
    "print(\"   ‚Ä¢ labels: ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏Ç‡πà‡∏≤‡∏ß (business, entertainment, politics, sport, tech)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a2cb95",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Step 2: Import Libraries\n",
    "\n",
    "‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° libraries ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö text classification ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8fcdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(df)\n",
    "print(f\"üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total:,} ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°\")\n",
    "print(f\"\\nüí° ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\")\n",
    "print(f\"   ‚Ä¢ Dataset ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß BBC ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {total:,} ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°\")\n",
    "print(f\"   ‚Ä¢ ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ 2 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: text (‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤) ‡πÅ‡∏•‡∏∞ labels (‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98632f0",
   "metadata": {},
   "source": [
    "## üìÇ Step 3: Load Data\n",
    "\n",
    "‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• BBC News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae61b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df['text']\n",
    "labels = df['labels']\n",
    "\n",
    "print(\"üì¶ ‡πÅ‡∏¢‡∏Å Features ‡πÅ‡∏•‡∏∞ Labels:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   ‚Ä¢ inputs (text): {len(inputs)} ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°\")\n",
    "print(f\"   ‚Ä¢ labels: {len(labels)} labels\")\n",
    "print(f\"\\nüí° ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:\")\n",
    "print(f\"   Text (30 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å): '{inputs.iloc[0][:30]}...'\")\n",
    "print(f\"   Label: '{labels.iloc[0]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cc412",
   "metadata": {},
   "source": [
    "### üìè ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b9588",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "labels.hist(figsize=(12, 6), edgecolor='black', alpha=0.7, color='steelblue')\n",
    "plt.title('üìä ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏Ç‡πà‡∏≤‡∏ß BBC', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏Ç‡πà‡∏≤‡∏ß', fontsize=11)\n",
    "plt.ylabel('‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°', fontsize=11)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(alpha=0.3, linestyle='--', axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà\n",
    "print(\"\\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà:\")\n",
    "print(\"=\" * 60)\n",
    "for label, count in labels.value_counts().items():\n",
    "    percent = count / len(labels) * 100\n",
    "    print(f\"   ‚Ä¢ {label:15s}: {count:4d} ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° ({percent:5.2f}%)\")\n",
    "\n",
    "print(\"\\nüí° ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:\")\n",
    "ratios = labels.value_counts() / len(labels)\n",
    "if ratios.max() - ratios.min() < 0.15:\n",
    "    print(\"   ‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏™‡∏°‡∏î‡∏∏‡∏• (Balanced Dataset)\")\n",
    "    print(\"   ‚úÖ Accuracy ‡πÄ‡∏õ‡πá‡∏ô metric ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏• (Imbalanced Dataset)\")\n",
    "    print(\"   ‚ö†Ô∏è ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ metrics ‡∏≠‡∏∑‡πà‡∏ô ‡πÄ‡∏ä‡πà‡∏ô F1-Score, Precision, Recall\")\n",
    "\n",
    "print(\"\\nüéØ ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "print(\"   ‚Ä¢ ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô\")\n",
    "print(\"   ‚Ä¢ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡πÉ‡∏î‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81526eba",
   "metadata": {},
   "source": [
    "### üì¶ ‡πÅ‡∏¢‡∏Å Features ‡πÅ‡∏•‡∏∞ Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, inputs_test, Ytrain, Ytest = train_test_split(\n",
    "    inputs, labels, random_state=123)\n",
    "\n",
    "print(\"üî™ ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train/Test:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   ‚Ä¢ Training Set: {len(inputs_train):,} ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° ({len(inputs_train)/len(inputs)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test Set:     {len(inputs_test):,} ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° ({len(inputs_test)/len(inputs)*100:.1f}%)\")\n",
    "print(f\"\\nüí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\")\n",
    "print(f\"   ‚Ä¢ Train: ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö)\")\n",
    "print(f\"   ‚Ä¢ Test: ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏ß‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û)\")\n",
    "print(f\"   ‚Ä¢ random_state=123: ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ (reproducible)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac807f52",
   "metadata": {},
   "source": [
    "### üìä ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Labels (Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8d4522",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "print(\"üî§ ‡∏™‡∏£‡πâ‡∏≤‡∏á CountVectorizer:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Vectorizer ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\n",
    "print(\"\\nüí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\")\n",
    "print(\"   ‚Ä¢ CountVectorizer ‡∏à‡∏∞‡πÅ‡∏õ‡∏•‡∏á text ‚Üí matrix ‡∏Ç‡∏≠‡∏á word counts\")\n",
    "print(\"   ‚Ä¢ ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∞‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô feature ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß\")\n",
    "print(\"   ‚Ä¢ ‡∏Ñ‡πà‡∏≤ = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏±‡πâ‡∏ô‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a3baf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî™ Step 4: Split Data (Train/Test)\n",
    "\n",
    "‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Training ‡πÅ‡∏•‡∏∞ Testing Sets (75/25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c199ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß ‡πÅ‡∏õ‡∏•‡∏á Text ‚Üí Matrix (Vectorization):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# fit_transform: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ vocabulary + ‡πÅ‡∏õ‡∏•‡∏á\n",
    "Xtrain = vectorizer.fit_transform(inputs_train)\n",
    "print(f\"‚úÖ Training set vectorized\")\n",
    "print(f\"   ‚Ä¢ Shape: {Xtrain.shape}\")\n",
    "print(f\"   ‚Ä¢ {Xtrain.shape[0]:,} documents √ó {Xtrain.shape[1]:,} features (‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå)\")\n",
    "\n",
    "# transform: ‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ vocabulary ‡πÄ‡∏î‡∏¥‡∏°\n",
    "Xtest = vectorizer.transform(inputs_test)\n",
    "print(f\"\\n‚úÖ Test set vectorized\")\n",
    "print(f\"   ‚Ä¢ Shape: {Xtest.shape}\")\n",
    "print(f\"   ‚Ä¢ {Xtest.shape[0]:,} documents √ó {Xtest.shape[1]:,} features\")\n",
    "\n",
    "print(f\"\\nüí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\")\n",
    "print(f\"   ‚Ä¢ Vocabulary size: {Xtrain.shape[1]:,} ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥\")\n",
    "print(f\"   ‚Ä¢ ‡πÅ‡∏ï‡πà‡∏•‡∏∞ document ‚Üí vector ‡∏Ç‡∏≠‡∏á {Xtrain.shape[1]:,} dimensions\")\n",
    "print(f\"   ‚Ä¢ fit_transform: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ vocabulary ‡∏à‡∏≤‡∏Å train set\")\n",
    "print(f\"   ‚Ä¢ transform: ‡πÉ‡∏ä‡πâ vocabulary ‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡∏±‡∏ö test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846f18a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî§ Step 5: Text Vectorization (‡πÅ‡∏õ‡∏•‡∏á Text ‚Üí Numbers)\n",
    "\n",
    "### üìù CountVectorizer\n",
    "\n",
    "**‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå**: ‡πÅ‡∏õ‡∏•‡∏á text ‡πÄ‡∏õ‡πá‡∏ô matrix ‡∏Ç‡∏≠‡∏á word counts\n",
    "\n",
    "**‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô**:\n",
    "1. ‡∏™‡∏£‡πâ‡∏≤‡∏á vocabulary (‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)\n",
    "2. ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ document ‡πÄ‡∏õ‡πá‡∏ô vector ‡∏Ç‡∏≠‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏õ‡∏£‡∏≤‡∏Å‡∏è\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:\n",
    "```\n",
    "Document 1: \"I love Python\"\n",
    "Document 2: \"Python is great\"\n",
    "\n",
    "Vocabulary: [\"I\", \"love\", \"Python\", \"is\", \"great\"]\n",
    "\n",
    "Vector 1: [1, 1, 1, 0, 0]  ‚Üí I=1, love=1, Python=1, is=0, great=0\n",
    "Vector 2: [0, 0, 1, 1, 1]  ‚Üí I=0, love=0, Python=1, is=1, great=1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Training Matrix:\")\n",
    "print(\"=\" * 60)\n",
    "print(Xtrain)\n",
    "print(f\"\\nüí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\")\n",
    "print(f\"   ‚Ä¢ Type: {type(Xtrain).__name__} (Sparse Matrix)\")\n",
    "print(f\"   ‚Ä¢ Shape: {Xtrain.shape}\")\n",
    "print(f\"   ‚Ä¢ Stored elements: {Xtrain.nnz:,} (‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô 0)\")\n",
    "print(f\"   ‚Ä¢ Sparsity: {(1 - Xtrain.nnz/np.prod(Xtrain.shape))*100:.2f}% (‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏õ‡πá‡∏ô 0)\")\n",
    "print(f\"\\nüéØ ‡∏ó‡∏≥‡πÑ‡∏°‡πÉ‡∏ä‡πâ Sparse Matrix?\")\n",
    "print(f\"   ‚Ä¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤ 0)\")\n",
    "print(f\"   ‚Ä¢ ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏≠‡∏á matrix ‡πÄ‡∏õ‡πá‡∏ô 0 (‡∏Ñ‡∏≥‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πà‡∏≤‡∏ß)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907ab81",
   "metadata": {},
   "source": [
    "### üîß Transform: ‡πÅ‡∏õ‡∏•‡∏á Text ‚Üí Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d936b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero = (Xtrain != 0).sum()\n",
    "print(f\"üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô 0: {non_zero:,}\")\n",
    "print(f\"\\nüí° ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:\")\n",
    "print(f\"   ‚Ä¢ ‡∏à‡∏≤‡∏Å {Xtrain.shape[0]:,} √ó {Xtrain.shape[1]:,} = {np.prod(Xtrain.shape):,} ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\")\n",
    "print(f\"   ‚Ä¢ ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô 0 ‡πÄ‡∏û‡∏µ‡∏¢‡∏á {non_zero:,} ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á\")\n",
    "print(f\"   ‚Ä¢ ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {np.prod(Xtrain.shape) - non_zero:,} ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a04fc2",
   "metadata": {},
   "source": [
    "### üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Matrix ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = (Xtrain != 0).sum() / np.prod(Xtrain.shape)\n",
    "print(f\"üìâ Sparsity Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   ‚Ä¢ Density (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ô‡∏≤‡πÅ‡∏ô‡πà‡∏ô): {sparsity:.6f} ({sparsity*100:.4f}%)\")\n",
    "print(f\"   ‚Ä¢ Sparsity (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ö‡∏≤‡∏ö‡∏≤‡∏á): {1-sparsity:.6f} ({(1-sparsity)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\")\n",
    "print(f\"   ‚Ä¢ ‡πÄ‡∏û‡∏µ‡∏¢‡∏á {sparsity*100:.4f}% ‡∏Ç‡∏≠‡∏á matrix ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô 0\")\n",
    "print(f\"   ‚Ä¢ {(1-sparsity)*100:.2f}% ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ 0\")\n",
    "print(f\"\\nüéØ ‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡πÄ‡∏ö‡∏≤‡∏ö‡∏≤‡∏á‡∏°‡∏≤‡∏Å?\")\n",
    "print(f\"   ‚Ä¢ ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {Xtrain.shape[1]:,} ‡∏Ñ‡∏≥\")\n",
    "print(f\"   ‚Ä¢ Sparse Matrix ‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory ‡∏°‡∏´‡∏≤‡∏®‡∏≤‡∏•!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0952ad17",
   "metadata": {},
   "source": [
    "### üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô 0 (Non-zero Elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèãÔ∏è Training Multinomial Naive Bayes Model...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "\n",
    "train_acc = model.score(Xtrain, Ytrain)\n",
    "test_acc = model.score(Xtest, Ytest)\n",
    "\n",
    "print(\"‚úÖ Training ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
    "print(f\"\\nüìä ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:\")\n",
    "print(f\"   ‚Ä¢ Train Accuracy: {train_acc:.6f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Test Accuracy:  {test_acc:.6f} ({test_acc*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüí° ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:\")\n",
    "if abs(train_acc - test_acc) < 0.02:\n",
    "    print(f\"   ‚úÖ Train ‚âà Test = ‡πÑ‡∏°‡πà Overfit (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ)\")\n",
    "elif train_acc > test_acc + 0.05:\n",
    "    print(f\"   ‚ö†Ô∏è Train >> Test = Overfit (‡∏à‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ\")\n",
    "\n",
    "if test_acc > 0.95:\n",
    "    print(f\"   ‚úÖ Accuracy > 95% = ‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°! üéâ\")\n",
    "elif test_acc > 0.90:\n",
    "    print(f\"   ‚úÖ Accuracy > 90% = ‡∏î‡∏µ‡∏°‡∏≤‡∏Å!\")\n",
    "elif test_acc > 0.80:\n",
    "    print(f\"   ‚úÖ Accuracy > 80% = ‡∏î‡∏µ‡∏û‡∏≠‡πÉ‡∏ä‡πâ\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Accuracy < 80% = ‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á\")\n",
    "\n",
    "print(f\"\\nüéØ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:\")\n",
    "print(f\"   ‚Ä¢ ‡∏à‡∏≤‡∏Å {len(Xtest)} ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏™‡∏≠‡∏ö\")\n",
    "print(f\"   ‚Ä¢ ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å ~{int(test_acc*len(Xtest))} ‡∏Ç‡πà‡∏≤‡∏ß\")\n",
    "print(f\"   ‚Ä¢ ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î ~{int((1-test_acc)*len(Xtest))} ‡∏Ç‡πà‡∏≤‡∏ß\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19d0b75",
   "metadata": {},
   "source": [
    "### üìâ Sparsity (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ö‡∏≤‡∏ö‡∏≤‡∏á)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ab591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "Ptest = model.predict(Xtest)\n",
    "\n",
    "# ‡∏ß‡∏≤‡∏î Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm_display = ConfusionMatrixDisplay.from_predictions(Ytest, Ptest, ax=ax, cmap='Blues', colorbar=True)\n",
    "ax.set_title('üéØ Confusion Matrix: BBC News Classification\\n(Test Set)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (Predicted Label)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('‡∏à‡∏£‡∏¥‡∏á (True Label)', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥\n",
    "cm = confusion_matrix(Ytest, Ptest)\n",
    "categories = sorted(Ytest.unique())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìñ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏≠‡πà‡∏≤‡∏ô Confusion Matrix:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚Ä¢ ‡πÅ‡∏ñ‡∏ß (‡πÅ‡∏ô‡∏ß‡∏ô‡∏≠‡∏ô) = Label ‡∏à‡∏£‡∏¥‡∏á (True Label)\")\n",
    "print(\"‚Ä¢ ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (‡πÅ‡∏ô‡∏ß‡∏ï‡∏±‡πâ‡∏á) = Label ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (Predicted Label)\")\n",
    "print(\"‚Ä¢ ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡πÅ‡∏¢‡∏á‡∏°‡∏∏‡∏° (diagonal) = ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å ‚úÖ\")\n",
    "print(\"‚Ä¢ ‡∏ô‡∏≠‡∏Å‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡πÅ‡∏¢‡∏á = ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î ‚ùå\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, cat in enumerate(categories):\n",
    "    total_true = cm[i].sum()\n",
    "    correct_pred = cm[i, i]\n",
    "    acc_per_class = (correct_pred / total_true * 100) if total_true > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüì∞ {cat.upper()}:\")\n",
    "    print(f\"   ‚Ä¢ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏£‡∏¥‡∏á: {total_true} ‡∏Ç‡πà‡∏≤‡∏ß\")\n",
    "    print(f\"   ‚Ä¢ ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å: {correct_pred} ‡∏Ç‡πà‡∏≤‡∏ß\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {acc_per_class:.2f}%\")\n",
    "    \n",
    "    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î\n",
    "    errors = []\n",
    "    for j, other_cat in enumerate(categories):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            errors.append(f\"{cm[i, j]} ‚Üí {other_cat}\")\n",
    "    if errors:\n",
    "        print(f\"   ‚Ä¢ ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î‡πÄ‡∏õ‡πá‡∏ô: {', '.join(errors)}\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î! üéâ\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üí° ‡∏™‡∏£‡∏∏‡∏õ:\")\n",
    "print(\"=\" * 70)\n",
    "total_correct = cm.diagonal().sum()\n",
    "total = cm.sum()\n",
    "print(f\"‚Ä¢ Accuracy ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°: {total_correct/total:.4f} ({total_correct/total*100:.2f}%)\")\n",
    "print(f\"‚Ä¢ ‡∏à‡∏≤‡∏Å {total} ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏™‡∏≠‡∏ö:\")\n",
    "print(f\"  - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å: {total_correct} ‡∏Ç‡πà‡∏≤‡∏ß ‚úÖ\")\n",
    "print(f\"  - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î: {total - total_correct} ‡∏Ç‡πà‡∏≤‡∏ß ‚ùå\")\n",
    "\n",
    "print(\"\\nüéØ ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "if cm.diagonal().min() / cm.sum(axis=1).max() > 0.85:\n",
    "    print(\"‚Ä¢ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏ó‡∏∏‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà ‚úÖ\")\n",
    "else:\n",
    "    print(\"‚Ä¢ ‡∏ö‡∏≤‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏≠‡∏∑‡πà‡∏ô\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c6ddf6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è Step 6: Train Multinomial Naive Bayes\n",
    "\n",
    "### üöÄ Training + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2855cf7f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Step 7: Confusion Matrix\n",
    "\n",
    "### üß© ‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏ö‡∏™‡∏ô (Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î\n",
    "misclassified_idx = np.where(Ptest != Ytest)[0]\n",
    "\n",
    "print(\"‚ùå ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î: {len(misclassified_idx)} ‡∏à‡∏≤‡∏Å {len(Ptest)} ({len(misclassified_idx)/len(Ptest)*100:.2f}%)\")\n",
    "\n",
    "if len(misclassified_idx) > 0:\n",
    "    i = np.random.choice(misclassified_idx)\n",
    "    true_label = Ytest.iloc[i]\n",
    "    pred_label = Ptest[i]\n",
    "    text = inputs_test.iloc[i]\n",
    "    \n",
    "    print(f\"\\nüì∞ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà {i+1}:\")\n",
    "    print(f\"   ‚Ä¢ ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á: {true_label}\")\n",
    "    print(f\"   ‚Ä¢ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {pred_label}\")\n",
    "    print(f\"   ‚Ä¢ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß (100 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å):\")\n",
    "    print(f\"     '{text[:100]}...'\")\n",
    "    \n",
    "    print(f\"\\nüí° ‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î?\")\n",
    "    print(f\"   ‚Ä¢ ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á {true_label} ‡πÅ‡∏•‡∏∞ {pred_label}\")\n",
    "    print(f\"   ‚Ä¢ ‡∏ö‡∏≤‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏Å‡∏±‡∏ô\")\n",
    "    print(f\"   ‚Ä¢ Naive assumption (‡∏Ñ‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞) ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏™‡∏°‡∏≠‡πÑ‡∏õ\")\n",
    "else:\n",
    "    print(\"\\nüéâ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î‡πÄ‡∏•‡∏¢! Perfect Classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac54bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Step 8: Analyze Misclassified Examples\n",
    "\n",
    "### ‚ùå ‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏î‡∏π shape ‡∏Ç‡∏≠‡∏á feature_log_prob_\n",
    "print(\"üìä Feature Log Probabilities:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   ‚Ä¢ Shape: {model.feature_log_prob_.shape}\")\n",
    "print(f\"   ‚Ä¢ {model.feature_log_prob_.shape[0]} classes √ó {model.feature_log_prob_.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nüí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\")\n",
    "print(f\"   ‚Ä¢ feature_log_prob_[i, j] = log P(word_j | class_i)\")\n",
    "print(f\"   ‚Ä¢ ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á = ‡∏Ñ‡∏≥‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏°‡∏≤‡∏Å‡πÉ‡∏ô class ‡∏ô‡∏µ‡πâ\")\n",
    "print(f\"   ‚Ä¢ ‡πÉ‡∏ä‡πâ‡∏´‡∏≤ top words ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61054ca8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ Step 9: Feature Analysis (‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà)\n",
    "\n",
    "### üìä ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Feature Log Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3008648",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.classes_\n",
    "print(\"üè∑Ô∏è Classes ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\")\n",
    "print(\"=\" * 60)\n",
    "for i, cls in enumerate(classes):\n",
    "    print(f\"   {i}. {cls}\")\n",
    "\n",
    "print(f\"\\nüí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\")\n",
    "print(f\"   ‚Ä¢ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ {len(classes)} ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà\")\n",
    "print(f\"   ‚Ä¢ Index ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô feature_log_prob_ array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439a6760",
   "metadata": {},
   "source": [
    "### üè∑Ô∏è Classes (‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.vocabulary_\n",
    "print(\"üìñ Vocabulary (‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   ‚Ä¢ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå: {len(vocab):,} ‡∏Ñ‡∏≥\")\n",
    "print(f\"   ‚Ä¢ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡∏Ñ‡∏≥‡πÅ‡∏£‡∏Å:\")\n",
    "for i, (word, idx) in enumerate(list(vocab.items())[:5]):\n",
    "    print(f\"     '{word}' ‚Üí index {idx}\")\n",
    "\n",
    "print(f\"\\nüí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\")\n",
    "print(f\"   ‚Ä¢ vocabulary_ ‡πÄ‡∏õ‡πá‡∏ô dictionary: word ‚Üí index\")\n",
    "print(f\"   ‚Ä¢ index ‡πÉ‡∏ä‡πâ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÉ‡∏ô matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5063353",
   "metadata": {},
   "source": [
    "### üìñ Vocabulary (‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18754da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"üî§ Feature Names (Array ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   ‚Ä¢ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: {len(feature_names):,} ‡∏Ñ‡∏≥\")\n",
    "print(f\"   ‚Ä¢ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 10 ‡∏Ñ‡∏≥‡πÅ‡∏£‡∏Å: {list(feature_names[:10])}\")\n",
    "\n",
    "print(f\"\\nüí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\")\n",
    "print(f\"   ‚Ä¢ get_feature_names_out() ‡∏Ñ‡∏∑‡∏ô array ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå\")\n",
    "print(f\"   ‚Ä¢ Index ‡∏Ç‡∏≠‡∏á array = index ‡πÉ‡∏ô matrix\")\n",
    "print(f\"   ‚Ä¢ ‡πÉ‡∏ä‡πâ‡πÅ‡∏õ‡∏•‡∏á index ‚Üí word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e6244",
   "metadata": {},
   "source": [
    "### üî§ Feature Names (‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"üîç ‡∏™‡∏£‡πâ‡∏≤‡∏á Index-to-Word Mapping:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ idx2word array ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\n",
    "print(f\"   ‚Ä¢ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: idx2word[0] = '{idx2word[0]}'\")\n",
    "print(f\"   ‚Ä¢ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: idx2word[100] = '{idx2word[100]}'\")\n",
    "\n",
    "print(f\"\\nüí° ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏´‡∏≤ Top Words ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏´‡∏≤ top 10 words ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ class\n",
    "print(\"üîù Top 10 Words ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for class_idx, class_name in enumerate(model.classes_):\n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° log probability ‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å ‚Üí ‡∏ô‡πâ‡∏≠‡∏¢\n",
    "    idx = np.argsort(-model.feature_log_prob_[class_idx])[:10]\n",
    "    top_words = idx2word[idx]\n",
    "    \n",
    "    print(f\"\\nüì∞ {class_name.upper()}:\")\n",
    "    print(f\"   Top 10 words: {list(top_words)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚Ä¢ ‡∏Ñ‡∏≥‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà\")\n",
    "print(\"‚Ä¢ argsort(-log_prob) = ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å ‚Üí ‡∏ô‡πâ‡∏≠‡∏¢\")\n",
    "print(\"‚Ä¢ ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ö‡πà‡∏≠‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏≠‡∏Å‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ô‡∏±‡πâ‡∏ô ‡πÜ\")\n",
    "\n",
    "print(\"\\nüéØ ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "print(\"‚Ä¢ ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\")\n",
    "print(\"‚Ä¢ ‡πÄ‡∏ä‡πà‡∏ô: sport ‡∏°‡∏µ 'match', 'win', 'players'\")\n",
    "print(\"‚Ä¢ ‡πÄ‡∏ä‡πà‡∏ô: tech ‡∏°‡∏µ 'software', 'technology', 'digital'\")\n",
    "print(\"‚Ä¢ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÑ‡∏î‡πâ‡∏î‡∏µ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f27caf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "\n",
    "### ‚úÖ Key Takeaways:\n",
    "\n",
    "1. **Text Classification Pipeline**:\n",
    "   - Text ‚Üí CountVectorizer ‚Üí Sparse Matrix ‚Üí MultinomialNB\n",
    "   - ‡πÅ‡∏õ‡∏•‡∏á text ‡πÄ‡∏õ‡πá‡∏ô word counts (bag-of-words)\n",
    "   - ‡πÉ‡∏ä‡πâ Sparse Matrix ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory\n",
    "\n",
    "2. **Multinomial NB**:\n",
    "   - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Count Data (word frequencies)\n",
    "   - Assume features (words) ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô\n",
    "   - ‡πÄ‡∏£‡πá‡∏ß + ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏Å‡∏±‡∏ö text data\n",
    "\n",
    "3. **BBC News Dataset**:\n",
    "   - ~2,225 ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß, 5 ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà\n",
    "   - Vocabulary: ~25,000+ ‡∏Ñ‡∏≥\n",
    "   - Accuracy: >95% (‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°!)\n",
    "\n",
    "4. **Sparse Matrix**:\n",
    "   - >99% ‡∏Ç‡∏≠‡∏á matrix ‡πÄ‡∏õ‡πá‡∏ô 0 (sparsity ~99.5%)\n",
    "   - ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory ‡∏°‡∏´‡∏≤‡∏®‡∏≤‡∏•\n",
    "   - sklearn ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö sparse matrix ‡πÑ‡∏î‡πâ‡∏î‡∏µ\n",
    "\n",
    "5. **Feature Analysis**:\n",
    "   - ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\n",
    "   - Top words ‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà\n",
    "   - ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏î‡πâ‡∏î‡∏µ\n",
    "\n",
    "### üí° ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\n",
    "\n",
    "**‡∏ó‡∏≥‡πÑ‡∏° Multinomial NB ‡∏ñ‡∏∂‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Text?**\n",
    "- Text ‡∏°‡∏µ discrete counts (0, 1, 2, 3, ...)\n",
    "- Multinomial distribution ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö count data\n",
    "- ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å (Linear time complexity)\n",
    "\n",
    "**‡∏ó‡∏≥‡πÑ‡∏° Accuracy ‡∏™‡∏π‡∏á?**\n",
    "- BBC News ‡∏°‡∏µ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (sport ‚â† tech)\n",
    "- ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å\n",
    "- ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏™‡∏°‡∏î‡∏∏‡∏• (balanced)\n",
    "\n",
    "**‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î**:\n",
    "- Naive assumption (‡∏Ñ‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞) ‚Üí ‡πÑ‡∏°‡πà‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏™‡∏°‡∏≠‡πÑ‡∏õ\n",
    "- ‡πÑ‡∏°‡πà‡∏à‡∏±‡∏ö word order (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≥)\n",
    "- ‡πÑ‡∏°‡πà‡∏à‡∏±‡∏ö context (‡∏ö‡∏£‡∏¥‡∏ö‡∏ó)\n",
    "\n",
    "### üöÄ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ (Advanced):\n",
    "\n",
    "1. **Improve Preprocessing**:\n",
    "   - Remove stopwords (the, a, an, is, ...)\n",
    "   - Stemming/Lemmatization (running ‚Üí run)\n",
    "   - N-grams (bigrams, trigrams)\n",
    "   - TF-IDF instead of counts\n",
    "\n",
    "2. **Try Other Models**:\n",
    "   - Logistic Regression\n",
    "   - SVM\n",
    "   - Random Forest\n",
    "   - Neural Networks (LSTM, BERT)\n",
    "\n",
    "3. **Evaluation**:\n",
    "   - Cross-validation\n",
    "   - Per-class metrics (Precision, Recall, F1)\n",
    "   - ROC curves\n",
    "\n",
    "4. **Deployment**:\n",
    "   - Save model (pickle/joblib)\n",
    "   - Create API (Flask/FastAPI)\n",
    "   - Real-time classification\n",
    "\n",
    "---\n",
    "\n",
    "## üìö ‡∏™‡∏£‡∏∏‡∏õ Mathematical Concepts:\n",
    "\n",
    "### Multinomial Naive Bayes:\n",
    "\n",
    "**Probability**:\n",
    "$$P(y|X) = \\frac{P(X|y) \\times P(y)}{P(X)}$$\n",
    "\n",
    "**Multinomial Likelihood**:\n",
    "$$P(X|y) = \\frac{(\\sum_i x_i)!}{\\prod_i x_i!} \\prod_i p_{yi}^{x_i}$$\n",
    "\n",
    "**Log-space (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô code)**:\n",
    "$$\\log P(y|X) = \\sum_{i} x_i \\log p_{yi} + \\log P(y)$$\n",
    "\n",
    "**Where**:\n",
    "- $x_i$ = count ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà $i$ ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "- $p_{yi}$ = ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà $i$ ‡πÉ‡∏ô class $y$\n",
    "- $P(y)$ = prior probability ‡∏Ç‡∏≠‡∏á class $y$\n",
    "\n",
    "**‡∏ó‡∏≥‡πÑ‡∏°‡πÉ‡∏ä‡πâ log?**\n",
    "- Probabilities ‡πÄ‡∏•‡πá‡∏Å‡∏°‡∏≤‡∏Å ‚Üí Underflow\n",
    "- Log ‡πÅ‡∏õ‡∏•‡∏á multiplication ‚Üí addition\n",
    "- Numerically stable ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039d090",
   "metadata": {},
   "source": [
    "### üîç ‡∏™‡∏£‡πâ‡∏≤‡∏á Index-to-Word Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da683a2",
   "metadata": {},
   "source": [
    "### üîù Top 10 Words ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà\n",
    "\n",
    "**‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå**: ‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
