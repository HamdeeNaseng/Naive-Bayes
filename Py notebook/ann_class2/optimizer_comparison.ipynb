{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c06b3c0",
   "metadata": {},
   "source": [
    "# ‚ö° ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û: Optimization Algorithms\n",
    "\n",
    "**‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå**: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß, ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥, ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£ converge** ‡∏Ç‡∏≠‡∏á Optimizers ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
    "\n",
    "**‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà**: `machine_learning_examples-master/ann_class2`\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Optimizers ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:\n",
    "\n",
    "| # | Optimizer | ‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö | ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô |\n",
    "|---|-----------|------------|--------|\n",
    "| 1 | **SGD** | `sgd.py` | ‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î, baseline |\n",
    "| 2 | **Momentum** | `momentum.py` | ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ SGD, ‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡πÄ‡∏î‡πâ‡∏á |\n",
    "| 3 | **RMSprop** | `rmsprop.py` | Adaptive learning rate |\n",
    "| 4 | **Adam** | `adam.py` | ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Momentum + RMSprop |\n",
    "\n",
    "---\n",
    "\n",
    "## üìä ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:\n",
    "\n",
    "### 1Ô∏è‚É£ **Convergence Speed** (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏π‡πà‡πÄ‡∏Ç‡πâ‡∏≤)\n",
    "- Loss ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô?\n",
    "- Accuracy ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô?\n",
    "\n",
    "### 2Ô∏è‚É£ **Final Performance** (‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)\n",
    "- Final Loss ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ\n",
    "- Final Accuracy ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ\n",
    "\n",
    "### 3Ô∏è‚É£ **Stability** (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£)\n",
    "- Loss curve ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏£‡∏∞‡πÄ‡∏î‡πâ‡∏á?\n",
    "- ‡∏°‡∏µ oscillations ‡∏°‡∏≤‡∏Å‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô?\n",
    "\n",
    "### 4Ô∏è‚É£ **Hyperparameter Sensitivity** (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ß‡∏ï‡πà‡∏≠ hyperparameters)\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏¢‡∏≤‡∏Å-‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô?\n",
    "- ‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å learning rate ‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô?\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏î‡∏™‡∏≠‡∏ö:\n",
    "\n",
    "**MNIST Digit Recognition** (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô adam.py, momentum.py):\n",
    "- Input: 784 pixels (28√ó28 images)\n",
    "- Output: 10 classes (digits 0-9)\n",
    "- Data: 60,000 training + 10,000 test samples\n",
    "- Architecture: **784 ‚Üí 300 ‚Üí 100 ‚Üí 10**\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö:\n",
    "\n",
    "1. ‡πÉ‡∏ä‡πâ **architecture ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å optimizer\n",
    "2. ‡πÉ‡∏ä‡πâ **learning rate ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô** (0.001) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∏‡∏ï‡∏¥‡∏ò‡∏£‡∏£‡∏°\n",
    "3. ‡πÉ‡∏ä‡πâ **batch size ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô** (100)\n",
    "4. Train **epochs ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô** (20 epochs)\n",
    "5. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö loss curves ‡πÅ‡∏•‡∏∞ accuracy curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f9944f",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Setup & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡πÑ‡∏ó‡∏¢\n",
    "font_path = '../../font/Prompt/Prompt-Regular.ttf'\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "fm.fontManager.addfont(font_path)\n",
    "plt.rcParams['font.family'] = font_prop.get_name()\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ‡πÄ‡∏û‡∏¥‡πà‡∏° path\n",
    "sys.path.append('../../machine_learning_examples-master')\n",
    "\n",
    "# Import utility functions\n",
    "from ann_class2.util import get_normalized_data, y2indicator\n",
    "\n",
    "print(\"‚úÖ Setup ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70edd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "print(\"üì• Loading MNIST dataset...\\n\")\n",
    "\n",
    "X, Y = get_normalized_data()\n",
    "\n",
    "# Split data\n",
    "X_train = X[:-1000]\n",
    "Y_train = Y[:-1000]\n",
    "X_test = X[-1000:]\n",
    "Y_test = Y[-1000:]\n",
    "\n",
    "# One-hot encode\n",
    "Y_train_ind = y2indicator(Y_train)\n",
    "Y_test_ind = y2indicator(Y_test)\n",
    "\n",
    "print(f\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"   ‚Ä¢ Training samples: {len(X_train)}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(X_test)}\")\n",
    "print(f\"   ‚Ä¢ Input features: {X_train.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Output classes: {Y_train_ind.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d2b12",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 2: Build Neural Network Class\n",
    "\n",
    "**Architecture**: 784 ‚Üí 300 ‚Üí 100 ‚Üí 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Neural Network with customizable optimizer\n",
    "    \n",
    "    Architecture: 784 ‚Üí 300 ‚Üí 100 ‚Üí 10\n",
    "    Activation: ReLU (hidden), Softmax (output)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, M1=300, M2=100):\n",
    "        self.M1 = M1  # Hidden layer 1 size\n",
    "        self.M2 = M2  # Hidden layer 2 size\n",
    "        \n",
    "    def fit(self, X, Y, Xtest, Ytest, optimizer='adam', \n",
    "            learning_rate=0.001, epochs=20, batch_size=100, \n",
    "            beta1=0.9, beta2=0.999, eps=1e-8, mu=0.9, decay_rate=0.999):\n",
    "        \"\"\"\n",
    "        Train the network\n",
    "        \n",
    "        Parameters:\n",
    "        - optimizer: 'sgd', 'momentum', 'rmsprop', 'adam'\n",
    "        - beta1: momentum parameter for Adam (default: 0.9)\n",
    "        - beta2: RMSprop parameter for Adam (default: 0.999)\n",
    "        - mu: momentum parameter for Momentum optimizer (default: 0.9)\n",
    "        - decay_rate: decay rate for RMSprop (default: 0.999)\n",
    "        \"\"\"\n",
    "        # Initialize\n",
    "        N, D = X.shape\n",
    "        K = Y.shape[1]\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.randn(D, self.M1) / np.sqrt(D)\n",
    "        self.b1 = np.zeros(self.M1)\n",
    "        self.W2 = np.random.randn(self.M1, self.M2) / np.sqrt(self.M1)\n",
    "        self.b2 = np.zeros(self.M2)\n",
    "        self.W3 = np.random.randn(self.M2, K) / np.sqrt(self.M2)\n",
    "        self.b3 = np.zeros(K)\n",
    "        \n",
    "        # Initialize optimizer-specific variables\n",
    "        if optimizer == 'momentum':\n",
    "            # Momentum velocities\n",
    "            self.vW1 = 0\n",
    "            self.vb1 = 0\n",
    "            self.vW2 = 0\n",
    "            self.vb2 = 0\n",
    "            self.vW3 = 0\n",
    "            self.vb3 = 0\n",
    "            \n",
    "        elif optimizer == 'rmsprop':\n",
    "            # RMSprop cache\n",
    "            self.cW1 = 1\n",
    "            self.cb1 = 1\n",
    "            self.cW2 = 1\n",
    "            self.cb2 = 1\n",
    "            self.cW3 = 1\n",
    "            self.cb3 = 1\n",
    "            \n",
    "        elif optimizer == 'adam':\n",
    "            # Adam: momentum + RMSprop\n",
    "            self.mW1 = 0\n",
    "            self.mb1 = 0\n",
    "            self.mW2 = 0\n",
    "            self.mb2 = 0\n",
    "            self.mW3 = 0\n",
    "            self.mb3 = 0\n",
    "            self.vW1 = 0\n",
    "            self.vb1 = 0\n",
    "            self.vW2 = 0\n",
    "            self.vb2 = 0\n",
    "            self.vW3 = 0\n",
    "            self.vb3 = 0\n",
    "        \n",
    "        # Track metrics\n",
    "        self.train_costs = []\n",
    "        self.test_costs = []\n",
    "        self.train_accuracies = []\n",
    "        self.test_accuracies = []\n",
    "        \n",
    "        # Training\n",
    "        n_batches = N // batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            X_shuffled, Y_shuffled = shuffle(X, Y)\n",
    "            \n",
    "            for batch_idx in range(n_batches):\n",
    "                # Get batch\n",
    "                start = batch_idx * batch_size\n",
    "                end = start + batch_size\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                Y_batch = Y_shuffled[start:end]\n",
    "                \n",
    "                # Forward pass\n",
    "                pY, Z1, Z2 = self.forward(X_batch)\n",
    "                \n",
    "                # Gradients\n",
    "                pY_Y = pY - Y_batch\n",
    "                \n",
    "                # Backpropagation\n",
    "                self.gW3 = Z2.T.dot(pY_Y)\n",
    "                self.gb3 = pY_Y.sum(axis=0)\n",
    "                \n",
    "                dZ2 = pY_Y.dot(self.W3.T) * (Z2 > 0)  # ReLU derivative\n",
    "                self.gW2 = Z1.T.dot(dZ2)\n",
    "                self.gb2 = dZ2.sum(axis=0)\n",
    "                \n",
    "                dZ1 = dZ2.dot(self.W2.T) * (Z1 > 0)  # ReLU derivative\n",
    "                self.gW1 = X_batch.T.dot(dZ1)\n",
    "                self.gb1 = dZ1.sum(axis=0)\n",
    "                \n",
    "                # Update weights based on optimizer\n",
    "                if optimizer == 'sgd':\n",
    "                    self.update_sgd(learning_rate)\n",
    "                elif optimizer == 'momentum':\n",
    "                    self.update_momentum(learning_rate, mu)\n",
    "                elif optimizer == 'rmsprop':\n",
    "                    self.update_rmsprop(learning_rate, decay_rate, eps)\n",
    "                elif optimizer == 'adam':\n",
    "                    self.update_adam(learning_rate, beta1, beta2, eps, epoch * n_batches + batch_idx + 1)\n",
    "            \n",
    "            # Calculate costs\n",
    "            pY_train = self.predict(X)\n",
    "            train_cost = self.cost(Y, pY_train)\n",
    "            train_acc = self.classification_rate(np.argmax(Y, axis=1), np.argmax(pY_train, axis=1))\n",
    "            \n",
    "            pY_test = self.predict(Xtest)\n",
    "            test_cost = self.cost(Ytest, pY_test)\n",
    "            test_acc = self.classification_rate(np.argmax(Ytest, axis=1), np.argmax(pY_test, axis=1))\n",
    "            \n",
    "            self.train_costs.append(train_cost)\n",
    "            self.test_costs.append(test_cost)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            self.test_accuracies.append(test_acc)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"Epoch {epoch:2d} | Train: {train_cost:.4f}, {train_acc:.4f} | Test: {test_cost:.4f}, {test_acc:.4f}\")\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        Z1 = np.maximum(0, X.dot(self.W1) + self.b1)  # ReLU\n",
    "        Z2 = np.maximum(0, Z1.dot(self.W2) + self.b2)  # ReLU\n",
    "        Y = self.softmax(Z2.dot(self.W3) + self.b3)\n",
    "        return Y, Z1, Z2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict probabilities\"\"\"\n",
    "        Y, _, _ = self.forward(X)\n",
    "        return Y\n",
    "    \n",
    "    def softmax(self, A):\n",
    "        \"\"\"Softmax activation\"\"\"\n",
    "        expA = np.exp(A - np.max(A, axis=1, keepdims=True))  # numerical stability\n",
    "        return expA / expA.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    def cost(self, T, Y):\n",
    "        \"\"\"Cross-entropy loss\"\"\"\n",
    "        return -(T * np.log(Y + 1e-10)).sum()\n",
    "    \n",
    "    def classification_rate(self, Y, P):\n",
    "        \"\"\"Classification accuracy\"\"\"\n",
    "        return np.mean(Y == P)\n",
    "    \n",
    "    # ===== Optimizer Update Rules =====\n",
    "    \n",
    "    def update_sgd(self, lr):\n",
    "        \"\"\"Stochastic Gradient Descent\"\"\"\n",
    "        self.W3 -= lr * self.gW3\n",
    "        self.b3 -= lr * self.gb3\n",
    "        self.W2 -= lr * self.gW2\n",
    "        self.b2 -= lr * self.gb2\n",
    "        self.W1 -= lr * self.gW1\n",
    "        self.b1 -= lr * self.gb1\n",
    "    \n",
    "    def update_momentum(self, lr, mu):\n",
    "        \"\"\"Momentum (SGD with velocity)\"\"\"\n",
    "        # Update velocities\n",
    "        self.vW3 = mu * self.vW3 - lr * self.gW3\n",
    "        self.vb3 = mu * self.vb3 - lr * self.gb3\n",
    "        self.vW2 = mu * self.vW2 - lr * self.gW2\n",
    "        self.vb2 = mu * self.vb2 - lr * self.gb2\n",
    "        self.vW1 = mu * self.vW1 - lr * self.gW1\n",
    "        self.vb1 = mu * self.vb1 - lr * self.gb1\n",
    "        \n",
    "        # Update weights\n",
    "        self.W3 += self.vW3\n",
    "        self.b3 += self.vb3\n",
    "        self.W2 += self.vW2\n",
    "        self.b2 += self.vb2\n",
    "        self.W1 += self.vW1\n",
    "        self.b1 += self.vb1\n",
    "    \n",
    "    def update_rmsprop(self, lr, decay, eps):\n",
    "        \"\"\"RMSprop (adaptive learning rate)\"\"\"\n",
    "        # Update cache (moving average of squared gradients)\n",
    "        self.cW3 = decay * self.cW3 + (1 - decay) * self.gW3 * self.gW3\n",
    "        self.cb3 = decay * self.cb3 + (1 - decay) * self.gb3 * self.gb3\n",
    "        self.cW2 = decay * self.cW2 + (1 - decay) * self.gW2 * self.gW2\n",
    "        self.cb2 = decay * self.cb2 + (1 - decay) * self.gb2 * self.gb2\n",
    "        self.cW1 = decay * self.cW1 + (1 - decay) * self.gW1 * self.gW1\n",
    "        self.cb1 = decay * self.cb1 + (1 - decay) * self.gb1 * self.gb1\n",
    "        \n",
    "        # Update weights (adaptive learning rate)\n",
    "        self.W3 -= lr * self.gW3 / (np.sqrt(self.cW3) + eps)\n",
    "        self.b3 -= lr * self.gb3 / (np.sqrt(self.cb3) + eps)\n",
    "        self.W2 -= lr * self.gW2 / (np.sqrt(self.cW2) + eps)\n",
    "        self.b2 -= lr * self.gb2 / (np.sqrt(self.cb2) + eps)\n",
    "        self.W1 -= lr * self.gW1 / (np.sqrt(self.cW1) + eps)\n",
    "        self.b1 -= lr * self.gb1 / (np.sqrt(self.cb1) + eps)\n",
    "    \n",
    "    def update_adam(self, lr, beta1, beta2, eps, t):\n",
    "        \"\"\"Adam (Momentum + RMSprop with bias correction)\"\"\"\n",
    "        # Update biased first moment (momentum)\n",
    "        self.mW3 = beta1 * self.mW3 + (1 - beta1) * self.gW3\n",
    "        self.mb3 = beta1 * self.mb3 + (1 - beta1) * self.gb3\n",
    "        self.mW2 = beta1 * self.mW2 + (1 - beta1) * self.gW2\n",
    "        self.mb2 = beta1 * self.mb2 + (1 - beta1) * self.gb2\n",
    "        self.mW1 = beta1 * self.mW1 + (1 - beta1) * self.gW1\n",
    "        self.mb1 = beta1 * self.mb1 + (1 - beta1) * self.gb1\n",
    "        \n",
    "        # Update biased second moment (RMSprop)\n",
    "        self.vW3 = beta2 * self.vW3 + (1 - beta2) * self.gW3 * self.gW3\n",
    "        self.vb3 = beta2 * self.vb3 + (1 - beta2) * self.gb3 * self.gb3\n",
    "        self.vW2 = beta2 * self.vW2 + (1 - beta2) * self.gW2 * self.gW2\n",
    "        self.vb2 = beta2 * self.vb2 + (1 - beta2) * self.gb2 * self.gb2\n",
    "        self.vW1 = beta2 * self.vW1 + (1 - beta2) * self.gW1 * self.gW1\n",
    "        self.vb1 = beta2 * self.vb1 + (1 - beta2) * self.gb1 * self.gb1\n",
    "        \n",
    "        # Bias correction\n",
    "        mW3_hat = self.mW3 / (1 - beta1**t)\n",
    "        mb3_hat = self.mb3 / (1 - beta1**t)\n",
    "        mW2_hat = self.mW2 / (1 - beta1**t)\n",
    "        mb2_hat = self.mb2 / (1 - beta1**t)\n",
    "        mW1_hat = self.mW1 / (1 - beta1**t)\n",
    "        mb1_hat = self.mb1 / (1 - beta1**t)\n",
    "        \n",
    "        vW3_hat = self.vW3 / (1 - beta2**t)\n",
    "        vb3_hat = self.vb3 / (1 - beta2**t)\n",
    "        vW2_hat = self.vW2 / (1 - beta2**t)\n",
    "        vb2_hat = self.vb2 / (1 - beta2**t)\n",
    "        vW1_hat = self.vW1 / (1 - beta2**t)\n",
    "        vb1_hat = self.vb1 / (1 - beta2**t)\n",
    "        \n",
    "        # Update weights\n",
    "        self.W3 -= lr * mW3_hat / (np.sqrt(vW3_hat) + eps)\n",
    "        self.b3 -= lr * mb3_hat / (np.sqrt(vb3_hat) + eps)\n",
    "        self.W2 -= lr * mW2_hat / (np.sqrt(vW2_hat) + eps)\n",
    "        self.b2 -= lr * mb2_hat / (np.sqrt(vb2_hat) + eps)\n",
    "        self.W1 -= lr * mW1_hat / (np.sqrt(vW1_hat) + eps)\n",
    "        self.b1 -= lr * mb1_hat / (np.sqrt(vb1_hat) + eps)\n",
    "\n",
    "print(\"‚úÖ Neural Network class ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27dd360",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Step 3: Train with Different Optimizers\n",
    "\n",
    "### 1Ô∏è‚É£ SGD (Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31554aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèÉ Training with SGD (Baseline)...\\n\")\n",
    "\n",
    "model_sgd = NeuralNetwork()\n",
    "model_sgd.fit(\n",
    "    X_train, Y_train_ind, X_test, Y_test_ind,\n",
    "    optimizer='sgd',\n",
    "    learning_rate=0.001,\n",
    "    epochs=20,\n",
    "    batch_size=100\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ SGD Training Complete!\")\n",
    "print(f\"   ‚Ä¢ Final Train Accuracy: {model_sgd.train_accuracies[-1]:.4f} ({model_sgd.train_accuracies[-1]*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Final Test Accuracy: {model_sgd.test_accuracies[-1]:.4f} ({model_sgd.test_accuracies[-1]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e9d6ea",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22135ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Training with Momentum...\\n\")\n",
    "\n",
    "model_momentum = NeuralNetwork()\n",
    "model_momentum.fit(\n",
    "    X_train, Y_train_ind, X_test, Y_test_ind,\n",
    "    optimizer='momentum',\n",
    "    learning_rate=0.001,\n",
    "    epochs=20,\n",
    "    batch_size=100,\n",
    "    mu=0.9  # momentum parameter\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Momentum Training Complete!\")\n",
    "print(f\"   ‚Ä¢ Final Train Accuracy: {model_momentum.train_accuracies[-1]:.4f} ({model_momentum.train_accuracies[-1]*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Final Test Accuracy: {model_momentum.test_accuracies[-1]:.4f} ({model_momentum.test_accuracies[-1]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eea75c4",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb174cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Training with RMSprop...\\n\")\n",
    "\n",
    "model_rmsprop = NeuralNetwork()\n",
    "model_rmsprop.fit(\n",
    "    X_train, Y_train_ind, X_test, Y_test_ind,\n",
    "    optimizer='rmsprop',\n",
    "    learning_rate=0.001,\n",
    "    epochs=20,\n",
    "    batch_size=100,\n",
    "    decay_rate=0.999  # RMSprop decay\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ RMSprop Training Complete!\")\n",
    "print(f\"   ‚Ä¢ Final Train Accuracy: {model_rmsprop.train_accuracies[-1]:.4f} ({model_rmsprop.train_accuracies[-1]*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Final Test Accuracy: {model_rmsprop.test_accuracies[-1]:.4f} ({model_rmsprop.test_accuracies[-1]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c385e95",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Adam (Adaptive Moment Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5147f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö° Training with Adam...\\n\")\n",
    "\n",
    "model_adam = NeuralNetwork()\n",
    "model_adam.fit(\n",
    "    X_train, Y_train_ind, X_test, Y_test_ind,\n",
    "    optimizer='adam',\n",
    "    learning_rate=0.001,\n",
    "    epochs=20,\n",
    "    batch_size=100,\n",
    "    beta1=0.9,    # momentum\n",
    "    beta2=0.999   # RMSprop\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Adam Training Complete!\")\n",
    "print(f\"   ‚Ä¢ Final Train Accuracy: {model_adam.train_accuracies[-1]:.4f} ({model_adam.train_accuracies[-1]*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Final Test Accuracy: {model_adam.test_accuracies[-1]:.4f} ({model_adam.test_accuracies[-1]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee1c31",
   "metadata": {},
   "source": [
    "## üìä Step 4: ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "\n",
    "### üèÜ Performance Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed2fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
    "comparison_data = {\n",
    "    'Optimizer': ['SGD', 'Momentum', 'RMSprop', 'Adam'],\n",
    "    'Train Accuracy': [\n",
    "        f\"{model_sgd.train_accuracies[-1]:.2%}\",\n",
    "        f\"{model_momentum.train_accuracies[-1]:.2%}\",\n",
    "        f\"{model_rmsprop.train_accuracies[-1]:.2%}\",\n",
    "        f\"{model_adam.train_accuracies[-1]:.2%}\"\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        f\"{model_sgd.test_accuracies[-1]:.2%}\",\n",
    "        f\"{model_momentum.test_accuracies[-1]:.2%}\",\n",
    "        f\"{model_rmsprop.test_accuracies[-1]:.2%}\",\n",
    "        f\"{model_adam.test_accuracies[-1]:.2%}\"\n",
    "    ],\n",
    "    'Final Train Loss': [\n",
    "        f\"{model_sgd.train_costs[-1]:.2f}\",\n",
    "        f\"{model_momentum.train_costs[-1]:.2f}\",\n",
    "        f\"{model_rmsprop.train_costs[-1]:.2f}\",\n",
    "        f\"{model_adam.train_costs[-1]:.2f}\"\n",
    "    ],\n",
    "    'Converge Speed': ['‚≠ê', '‚≠ê‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê'],\n",
    "    'Stability': ['‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê'],\n",
    "    'Ease of Tuning': ['‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê']\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ PERFORMANCE COMPARISON: Optimization Algorithms\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca95a4",
   "metadata": {},
   "source": [
    "### üìà Visualize Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss Curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Train Loss\n",
    "axes[0].plot(model_sgd.train_costs, label='SGD', linewidth=2, alpha=0.8)\n",
    "axes[0].plot(model_momentum.train_costs, label='Momentum', linewidth=2, alpha=0.8)\n",
    "axes[0].plot(model_rmsprop.train_costs, label='RMSprop', linewidth=2, alpha=0.8)\n",
    "axes[0].plot(model_adam.train_costs, label='Adam', linewidth=2, alpha=0.8)\n",
    "axes[0].set_title('üìâ Training Loss', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# Plot 2: Test Loss\n",
    "axes[1].plot(model_sgd.test_costs, label='SGD', linewidth=2, alpha=0.8)\n",
    "axes[1].plot(model_momentum.test_costs, label='Momentum', linewidth=2, alpha=0.8)\n",
    "axes[1].plot(model_rmsprop.test_costs, label='RMSprop', linewidth=2, alpha=0.8)\n",
    "axes[1].plot(model_adam.test_costs, label='Adam', linewidth=2, alpha=0.8)\n",
    "axes[1].set_title('üìâ Test Loss', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('Loss', fontsize=11)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.suptitle('Loss Curves: SGD vs Momentum vs RMSprop vs Adam', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Adam ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy Curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Train Accuracy\n",
    "axes[0].plot(model_sgd.train_accuracies, label='SGD', linewidth=2, alpha=0.8)\n",
    "axes[0].plot(model_momentum.train_accuracies, label='Momentum', linewidth=2, alpha=0.8)\n",
    "axes[0].plot(model_rmsprop.train_accuracies, label='RMSprop', linewidth=2, alpha=0.8)\n",
    "axes[0].plot(model_adam.train_accuracies, label='Adam', linewidth=2, alpha=0.8)\n",
    "axes[0].set_title('üìà Training Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].set_ylim([0.8, 1.0])\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# Plot 2: Test Accuracy\n",
    "axes[1].plot(model_sgd.test_accuracies, label='SGD', linewidth=2, alpha=0.8)\n",
    "axes[1].plot(model_momentum.test_accuracies, label='Momentum', linewidth=2, alpha=0.8)\n",
    "axes[1].plot(model_rmsprop.test_accuracies, label='RMSprop', linewidth=2, alpha=0.8)\n",
    "axes[1].plot(model_adam.test_accuracies, label='Adam', linewidth=2, alpha=0.8)\n",
    "axes[1].set_title('üìà Test Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[1].set_ylim([0.8, 1.0])\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.suptitle('Accuracy Curves: SGD vs Momentum vs RMSprop vs Adam', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Adam ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏™‡∏π‡∏á!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart: Final Test Accuracy Comparison\n",
    "optimizers = ['SGD', 'Momentum', 'RMSprop', 'Adam']\n",
    "test_accuracies = [\n",
    "    model_sgd.test_accuracies[-1],\n",
    "    model_momentum.test_accuracies[-1],\n",
    "    model_rmsprop.test_accuracies[-1],\n",
    "    model_adam.test_accuracies[-1]\n",
    "]\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "bars = plt.bar(optimizers, test_accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, test_accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{acc:.2%}',\n",
    "             ha='center', va='bottom', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.title('üèÜ Final Test Accuracy Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.ylim([0.9, 1.0])\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Adam ‡πÉ‡∏´‡πâ accuracy ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75e3054",
   "metadata": {},
   "source": [
    "## üéì ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Optimizer ‡πÑ‡∏´‡∏ô‡∏î‡∏µ?\n",
    "\n",
    "### üèÜ ‡∏ú‡∏π‡πâ‡∏ä‡∏ô‡∏∞‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏î‡πâ‡∏≤‡∏ô:\n",
    "\n",
    "#### 1Ô∏è‚É£ **Convergence Speed** (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏π‡πà‡πÄ‡∏Ç‡πâ‡∏≤) ‚ö°\n",
    "**ü•á Winner: Adam**\n",
    "- ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÅ‡∏£‡∏Å\n",
    "- Combines Momentum + RMSprop\n",
    "- **Ranking**: Adam > RMSprop > Momentum > SGD\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ **Final Performance** (‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢) üéØ\n",
    "**ü•á Winner: Adam / RMSprop (‡πÄ‡∏™‡∏°‡∏≠‡∏Å‡∏±‡∏ô)**\n",
    "- ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà‡πÑ‡∏î‡πâ Test Accuracy ~97-98%\n",
    "- Momentum ~96-97%\n",
    "- SGD ~95-96%\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ **Stability** (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£) üîí\n",
    "**ü•á Winner: Adam**\n",
    "- Loss curve ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "- Oscillations ‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "- **Ranking**: Adam > RMSprop > Momentum > SGD\n",
    "\n",
    "---\n",
    "\n",
    "#### 4Ô∏è‚É£ **Ease of Tuning** (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤) üéõÔ∏è\n",
    "**ü•á Winner: Adam**\n",
    "- Default parameters ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏Å‡∏£‡∏ì‡∏µ\n",
    "- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö learning rate ‡∏ö‡πà‡∏≠‡∏¢\n",
    "- **Ranking**: Adam > RMSprop > Momentum > SGD\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Algorithm Details:\n",
    "\n",
    "#### üî¥ **SGD** (Stochastic Gradient Descent)\n",
    "```python\n",
    "W = W - learning_rate * gradient\n",
    "```\n",
    "**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**:\n",
    "- ‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢\n",
    "- ‡πÉ‡∏ä‡πâ memory ‡∏ô‡πâ‡∏≠‡∏¢\n",
    "\n",
    "**‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢**:\n",
    "- ‡∏ä‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "- Oscillates ‡∏°‡∏≤‡∏Å\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á learning rate ‡πÉ‡∏´‡πâ‡∏î‡∏µ\n",
    "\n",
    "---\n",
    "\n",
    "#### üîµ **Momentum**\n",
    "```python\n",
    "v = mu * v - learning_rate * gradient\n",
    "W = W + v\n",
    "```\n",
    "**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**:\n",
    "- ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ SGD ‡∏°‡∏≤‡∏Å\n",
    "- ‡∏•‡∏î oscillations\n",
    "- \"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏â‡∏∑‡πà‡∏≠‡∏¢\" ‡∏ä‡πà‡∏ß‡∏¢‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏•‡∏∏‡∏°‡πÄ‡∏•‡πá‡∏Å‡πÜ\n",
    "\n",
    "**‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢**:\n",
    "- ‡∏¢‡∏±‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á learning rate\n",
    "- ‡∏°‡∏µ hyperparameter ‡πÄ‡∏û‡∏¥‡πà‡∏° (mu)\n",
    "\n",
    "**Typical**: `mu = 0.9` (‡πÉ‡∏ä‡πâ gradient 90% ‡∏à‡∏≤‡∏Å‡∏£‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô)\n",
    "\n",
    "---\n",
    "\n",
    "#### üü¢ **RMSprop** (Root Mean Square Propagation)\n",
    "```python\n",
    "cache = decay * cache + (1-decay) * gradient¬≤\n",
    "W = W - learning_rate * gradient / (sqrt(cache) + eps)\n",
    "```\n",
    "**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**:\n",
    "- **Adaptive learning rate** (‡πÅ‡∏ï‡πà‡∏•‡∏∞ parameter ‡πÉ‡∏ä‡πâ lr ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô)\n",
    "- ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£\n",
    "- ‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö non-stationary objectives\n",
    "\n",
    "**‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢**:\n",
    "- ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤ Momentum\n",
    "- ‡πÉ‡∏ä‡πâ memory ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤\n",
    "\n",
    "**Typical**: `decay = 0.999`, `eps = 1e-8`\n",
    "\n",
    "---\n",
    "\n",
    "#### üü† **Adam** (Adaptive Moment Estimation)\n",
    "```python\n",
    "# First moment (momentum)\n",
    "m = beta1 * m + (1-beta1) * gradient\n",
    "\n",
    "# Second moment (RMSprop)\n",
    "v = beta2 * v + (1-beta2) * gradient¬≤\n",
    "\n",
    "# Bias correction\n",
    "m_hat = m / (1 - beta1^t)\n",
    "v_hat = v / (1 - beta2^t)\n",
    "\n",
    "# Update\n",
    "W = W - learning_rate * m_hat / (sqrt(v_hat) + eps)\n",
    "```\n",
    "**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**:\n",
    "- **Best of both worlds** (Momentum + RMSprop)\n",
    "- Bias correction (‡∏î‡∏µ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á training)\n",
    "- Default parameters ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏Å‡∏£‡∏ì‡∏µ\n",
    "- **Most popular** ‡πÉ‡∏ô Deep Learning\n",
    "\n",
    "**‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢**:\n",
    "- ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "- ‡πÉ‡∏ä‡πâ memory ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "- ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á generalization ‡πÅ‡∏¢‡πà‡∏Å‡∏ß‡πà‡∏≤ SGD (‡πÉ‡∏ô specific cases)\n",
    "\n",
    "**Typical**: `beta1 = 0.9`, `beta2 = 0.999`, `eps = 1e-8`\n",
    "\n",
    "---\n",
    "\n",
    "### üìä ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏™‡∏£‡∏∏‡∏õ:\n",
    "\n",
    "| Criteria | SGD | Momentum | RMSprop | Adam |\n",
    "|----------|-----|----------|---------|------|\n",
    "| **Speed** | ü•â ‡∏ä‡πâ‡∏≤ | ü•à ‡πÄ‡∏£‡πá‡∏ß | ü•á ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å | ü•á ‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î |\n",
    "| **Accuracy** | ü•â 95-96% | ü•à 96-97% | ü•á 97-98% | ü•á 97-98% |\n",
    "| **Stability** | ü•â ‡∏Å‡∏£‡∏∞‡πÄ‡∏î‡πâ‡∏á‡∏°‡∏≤‡∏Å | ü•à ‡∏Å‡∏•‡∏≤‡∏á | ü•á ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ | ü•á ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î |\n",
    "| **Ease of Use** | ü•â ‡∏¢‡∏≤‡∏Å | ü•à ‡∏Å‡∏•‡∏≤‡∏á | ü•á ‡∏á‡πà‡∏≤‡∏¢ | ü•á ‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î |\n",
    "| **Memory** | ü•á ‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î | ü•à ‡∏ô‡πâ‡∏≠‡∏¢ | ü•à ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á | ü•â ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î |\n",
    "| **Complexity** | ü•á ‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î | ü•à ‡∏á‡πà‡∏≤‡∏¢ | ü•à ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á | ü•â ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:\n",
    "\n",
    "#### ‚úÖ **‡πÉ‡∏ä‡πâ Adam ‡πÄ‡∏°‡∏∑‡πà‡∏≠**:\n",
    "- üöÄ ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ **convergence ‡πÄ‡∏£‡πá‡∏ß**\n",
    "- üéØ ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ **accuracy ‡∏™‡∏π‡∏á**\n",
    "- üîß ‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏õ‡∏£‡∏±‡∏ö hyperparameters ‡πÄ‡∏¢‡∏≠‡∏∞\n",
    "- üíª ‡∏ó‡∏≥ **Deep Learning** ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà\n",
    "- üåü **Default choice** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\n",
    "\n",
    "#### ‚úÖ **‡πÉ‡∏ä‡πâ RMSprop ‡πÄ‡∏°‡∏∑‡πà‡∏≠**:\n",
    "- üìä ‡∏ó‡∏≥ **RNN** (Recurrent Neural Networks)\n",
    "- üîÑ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• **non-stationary** (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏•‡∏≠‡∏î)\n",
    "- ‚ö° ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ memory ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ Adam\n",
    "\n",
    "#### ‚úÖ **‡πÉ‡∏ä‡πâ Momentum ‡πÄ‡∏°‡∏∑‡πà‡∏≠**:\n",
    "- üéì ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ optimization algorithms\n",
    "- üíæ Memory ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡πÉ‡∏ä‡πâ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ Adam)\n",
    "- üî¨ Research: ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à dynamics ‡∏Ç‡∏≠‡∏á training\n",
    "\n",
    "#### ‚úÖ **‡πÉ‡∏ä‡πâ SGD ‡πÄ‡∏°‡∏∑‡πà‡∏≠**:\n",
    "- üìö ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô\n",
    "- üß† ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à optimization ‡∏•‡∏∂‡∏Å‡πÜ\n",
    "- üéØ **Better generalization** ‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡∏Å‡∏£‡∏ì‡∏µ (with good learning rate schedule)\n",
    "- üìù Baseline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
    "\n",
    "---\n",
    "\n",
    "### üîë Key Insights:\n",
    "\n",
    "#### üí° **Why is Adam so good?**\n",
    "1. **Adaptive Learning Rate**: ‡πÅ‡∏ï‡πà‡∏•‡∏∞ parameter ‡∏°‡∏µ lr ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á\n",
    "2. **Momentum**: ‡∏ä‡πà‡∏ß‡∏¢‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏•‡∏∏‡∏°‡πÄ‡∏•‡πá‡∏Å‡πÜ, ‡πÄ‡∏£‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß\n",
    "3. **Bias Correction**: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç bias ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á training\n",
    "4. **Robust**: ‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô‡∏ï‡πà‡∏≠ learning rate ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà optimal\n",
    "\n",
    "#### üí° **Why not always use Adam?**\n",
    "- ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á **SGD with momentum + learning rate schedule** generalize ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤\n",
    "- Adam ‡∏≠‡∏≤‡∏à‡∏à‡∏∞ **overfit** ‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô specific cases\n",
    "- Research papers ‡∏ö‡∏≤‡∏á‡∏ä‡∏¥‡πâ‡∏ô‡∏û‡∏ö‡∏ß‡πà‡∏≤ SGD ‡πÑ‡∏î‡πâ test accuracy ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ (‡πÅ‡∏ï‡πà train ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô‡πÜ:\n",
    "\n",
    "> **\"Adam is the Swiss Army Knife of Optimizers!\"**\n",
    "\n",
    "**‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö**:\n",
    "- ‚úÖ **Convergence**: Adam > RMSprop > Momentum > SGD\n",
    "- ‚úÖ **Accuracy**: Adam ‚âà RMSprop > Momentum > SGD\n",
    "- ‚úÖ **Stability**: Adam > RMSprop > Momentum > SGD\n",
    "- ‚úÖ **Ease**: Adam > RMSprop > Momentum > SGD\n",
    "\n",
    "**‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥**:\n",
    "1. **Default**: ‡πÉ‡∏ä‡πâ **Adam** (lr=0.001, beta1=0.9, beta2=0.999)\n",
    "2. **RNN**: ‡πÉ‡∏ä‡πâ **RMSprop**\n",
    "3. **Research**: ‡∏•‡∏≠‡∏á **SGD + Momentum + LR Schedule**\n",
    "4. **Learning**: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å SGD ‚Üí Momentum ‚Üí RMSprop ‚Üí Adam\n",
    "\n",
    "‚ú® **Happy Optimizing!** ‚ú®"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
